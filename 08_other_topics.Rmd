---
title: 
author: "Robert Ness"
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Other stuff


## Recap
* On the difference between covariate adjustment and do-calculus
  * Covariate adjustment motivation: When quantify the casual effect of X->Y, there might be an indirect path, e.g. Y<-Z->X, then we d-separate the X->Y path by condition on Z
  * Z acts like an AND gate for X
  * Covariate adjustment allows us to predict the results of an intervention.
    * Requires we observe the variables required to do the adjustment
    * Covariate adjustment is taking a weighted average of an effect over every combination of strata in the adjustment set.  This is practically difficult (in terms of computation and expectation) if the number of strata is large.  
        * If Z is discrete, e.g. Z={$Z_1,Z_2,Z_3$} is the adjustment set, and suppose $Z_1$ has 3 levels, $Z_2$ has 3 levels, $Z_3$ has 2 levels, then we have to consider 18 levels of covariate adjustments in total. That presents practical challenges. 
        * If Z is continuous, then you have to integrate, which has its own set of practical challenges.
  * Do-calculus is simulation of an intervention
    * When you don't have the variables neccessary to do covariate adjustment, you can still use do-calculus


## How to simulating adjustment using a propensity score function and inverse probability weighting
* This is useful when adjusting over all the strata in the adjustment set is practically difficult
* 
* Adjustment formula: 
$$P(Y=y|do(X = x)) = \sum_{Z} P(Y= y|X = x, Z = z)P(Z = z)$$
* Looking just at $P(Y= y|X = x, Z = z)$, Baye's rule tells us that:
$$P(Y= y|X = x, Z = z) = \frac{P(X = x, Y= y, Z = z)}{P(X = x, Z = z)}$$
* Bring back $P(Z = z)$
\begin{align}
P(Y= y|X = x, Z = z)P(Z=z) &= \frac{P(X = x, Y= y, Z = z)}{P(X = x, Z = z)}P(Z=z) \\
&= \frac{P(X = x, Y= y, Z = z)}{P(X = x| Z = z)P(Z=z)}P(Z=z) \\
&= \frac{P(X = x, Y= y, Z = z)}{P(X = x| Z = z)}
\end{align}
* Therefore we can rewrite the adjustment formula as:
$$P(Y=y|do(X = x)) = \sum_z  \frac{P(X = x, Y= y, Z = z)}{P(X = x| Z = z)}$$
*{P(X = x| Z = z)}* is consistent. 
* Suppose we are able to estimate a propensity score function $g(x, z) = P(X = x | Z = z)$
* Then we can estimate $P(Y = y|do(X =x))$ using the following *inverse probability weighting* algorithm 

```
# using Pyro-ish code
# M is desired number of samples

samples = []
weights = []
for i in M:
      x, y, z = model()
      sample_prob = model.prob(x, y, z)
      propensity_score = g(x, z)
      weight = sample_prob / propensity_score
      // only need to sample, more efficient and feasible than calculating pdf
      samples.append((x, y, z))
      weights.append(weight)
// resample according to new weights
new_samples = resample(samples, weights = weights)

```
* It is called inverse probability weighting because you multiply the joint probability of a sample by the inverse of a probability, in this case $g(x, z) = P(X = x|Z =z)$
* The frequencies in `new_samples` is such that you can estimate $P(Y=y|do(X = x)$ with $\hat{p}(Y = y|X = x)$, where $\hat{p}$ is a proportion in `new_samples`.

## Structural causal models

* Recall Laplace's demon example
* As we mentioned before, a structural causal model is a deterministic extention to $\mathbb{C}$ has a causal DAG $\mathbb{D}$.  Assume there are $J$ random variables in the DAG.
*The difference between deterministic and probabilistic assignment: e.g. deterministic -> $x_i=f_i(Pa_i,N_i)$ while probabilistic -> $x_i$~$P(x_i|Pa_i)$, in other words, deterministic assignment gives a fixed value but probabilistic gives a random value extracted from a distribution.
* Each varible in the DAG is paired with on independent random variables called exogenous noise terms, I will call them noise terms
* A distribution $P_{\mathbf{N}}^{\mathbb{C}}$ on independent **noise** random variables $\mathbf{N} = \{N_i; i \in J \}$
* The value of each variable is set deterministically by a function $f_i$ for the ith random variable called a structural assignments, such that
$X_i = f_i(\mathbf{PA}_{\mathbb{C}, i}, N_i), \forall i \in J$
where $\mathbf{PA}_{\mathbb{C}, i} \subseteq \mathbf{X} \setminus X_i$ are the parents of $X_i$ in $\mathbb{D}$.
* Some draw the noise terms, I usually do not.
* $\mathbb{C}$ is a generative model that entails $P^{\mathbb{G}}$, the same observational distribution as $\mathbb{G}$
* These are going to allow us to compute counterfactuals, they are on the highest rung of the ladder.
* They are not the only model on the top rung.  In a subsequent class, I will introduce some generalizations of structural causal models to open universe models.

## Causal inference in linear systems
* So far we have focused on covariate adjustment with discrete variables.
* We've avoided continuous variables generally for a few reasons.
  * Setting an intervention to point on a continuous domain seems weird. Why $do(X = 1.0)$ and not $do(X = 1.00001)$?
  * Integration and Bayesian probability math is practically challenging.
  * The math of course is simpler when you use linear modeling with Gaussian distributions.  However, this class casts causal modeling as an extention of generative machine learning; in cutting-edge generative machine learning you generally don't see a lot of linear modelling.
* However, we do touch on a few cases fundamental topics that come up in the causal linear modeling literature.

### Covariate adjustment example: Continuous adjustment

* We have been talking about thinking of causal effects as differences, what might this look like in the continuous case? $\frac{d}{dx}E^{\mathbb{M};do(X:=x)}(Y)$
  * Linear case -- Z in valid adjustment set
  * Nonlinear case: Monte Carlo Sampler.  Recall that. $\frac{d f(x)}{dx} = \lim_{\delta \rightarrow 0} \frac{f(x + \delta) -f(x)}{\delta}$

### Instrumental variables

* Consider a structural causal model with the following DAG ![instrumental](fig/instrumental.png)
* Consider the structural assignment for Y: $Y := \alpha X + \delta Z + N_Y$
* We are interested in the causal effect $\alpha$.  Let $\hat{\alpha}$ be our least-squares estimate of $\alpha$. Here confounding shows up as a bias in the standard regression estimator $\alpha$: $$E(\hat{\alpha}) = \frac{\text{cov}(X, Y)}{\text{var}(Y)} = \frac{\alpha \text{var}(X) + \delta \gamma \text{var}(Z)}{\text{var}(X)} =  \alpha + \frac{\delta \gamma \text{var}(Z)}{\text{var}(X)} \neq \alpha $$
* An instumental variable $I$ for $X, Y$ is one where:
  1. $I$ is independent of $Z$
  2. $I$ is not independent of $X$
  3. $I$ affects $Y$ only through $X$
* Two-stage least squares estimation using an instrumental variable algorithm:
  1. Regress X on Z and get $\hat{\beta}$ estimate of $\beta$
  2. Regress Y on the predicted values of the first regression $\hat{\beta}Z$
  3. The coefficient of $\hat{\beta}Z$ becomes is a consistent estimate of $\alpha$.

* Algorithm (R code)
```
fit1=lm(y~x)
resid=fit1.resid()
if (corr(x,resid)>epsilon) // confounding problem
    z=get_z() // use z to explain away the correlation between X and residence
    if (corr(x,z)>xi)
    //there is some backdoor path 
        fit2=lm(x~z)
        x_hat=predict(fit2)
        fit3=lm(y~x_hat)
        return (coef(fit3)$x)
```
* Statistical intuition:
  * Looking at the stuctural assignment for $X$: $$X:= \beta I + \gamma Z + N_X$$
  * Since $Z$ and $N_X$ are independent of $I$, then covariance between $Z, N_X, \hat{\gamma}$ is 0.  So we can treat $\gamma Z + N_X$ as a big noise term, and treat $\beta Z$ as a stand in for X.
  * We essentially modifiy Y's to be  : $$Y := \alpha (\beta Z) + (\alpha \gamma + \delta)Z + N_Y$$ and fit it using least-squares.
  
## Counterfactuals
* Notation
* $P_Y^{M|x=1,do(x=0)}(Y=1)$ -> In real world x=1, and we want to know what happen to y=1 if x=0
* use conditioning for real evidence and intervention for counterfactual
* e.g. In the dataset, we have x=a or b, for one x=a, what will happen to y if x=b
* e.g. Mueller's investigation: 'If we had confidence the President did not commit a crime, we would have said so'
    + speech: say Trump is innocent, belief: believe Trump is innocent
    + $P_{speech}^{speech=0, belief=0, do(belief=1)}(speech=1)$
    + current reality: don't say Trump is innocent, don't believe Trump is innocent
    + counterfactual / simulation: believe Trump is innocent

* Reasoning through inference algorithm with SMC:  Eye disease model
\begin{align}
 T &= N_T \\
 B &= T * N_B + (1-T)*(1-N_B) \\
  N_T ~ Ber(.5),& N_B ~ Ber(.01) 
\end{align}

_Note_: $T * N_B$ means patient will go blind if given treatment, $(1-T)*(1-N_B)$ means patient will go blind if given no treatment.

* Suppose patient with poor eyesight comes to the hospital and goes blind (B=1) after the doctor gives treatment (T=1).  
* We ask "what would have happened had the doctor administered treatment T = 0?"
* B = T = 1 means the $N_B$ was 1.
* Given $N_B$ equals 1, we calculate the effect of $do(T = 0)$ under new model
\begin{align}
 T &= 1\\
 B &= T * 1 + (1-T)*(1-1) = T \\
\end{align}

* Inspired by book 'Causal Inference in Statistics - a Primer (Pearl, Glymour, Jewell)' 4.2.4: 
    * Abduction: Use evidence $T = 1, B = 1$ to determine the value of $N_B$.
    * Prediction: Use the M and the value of T = 0 (counterfactual) to compute the value of B,
the consequence of the counterfactual.

## Bayesian counterfactual algorithm with SMCs in Pyro
1. Condition on observed data
2. Infer the noise terms
3. Apply do operator
4. Forward from noise posterior after having applied do operation.

Algorithm:
```
noise={N_y:Ber(P_y),N_x:Ber(P_x}
def model(noise):
    N_y=sample(noise[N_y])
    N_x=sample(noise[N_x])
    x=f_x(N_x)
    y=f_x(x,N_y)
    return x,y
condition_model=pyro.condition(model,{x:0,y:0})
do_model=pyro.do(model,{x:1})
noise_posterior=infer(condition_model,N_x,N_y)
do_model(noise_posterior)
```
## Potential outcomes framework

* Also called Rubin causal model, widely used in the social sciences, and as a result it is the one most commonly used in the tech industry.  This is because the big companies tend to higher economists such as Hal Varian and Susan Athey, as well as computationally savvey social scientists to address causal problems (auctions, recommendation problems, etc.).
* Reformulating blindness problem as potential outcomes
* Let $B_{u}^{T = 1}$ represent the outcome patient $u$ (going blind $B=1$ or getting cured $B=0$) if she recieves treatment $T=1$. $B_{u}^{T = 1} = 1$ the outcome for $u$ under $T=1$ is blindness, $B_{u}^{T = 1} = 0$ is no blindness.
* **The fundamental problem of causal inference**: To get a causal effect, we want $B_{u}^{T = 1} - B_{u}^{T = 0}$.  So $B_{u}^{T = 1}$ and $B_{u}^{T = 0}$ are both variables we want to know.  However, for each unit $u$ we can never observe both of these at the same time.  So one of these we call the "observed" outcome, and one of these we call the counterfactual outcome.
* Standard assumption in the framework is SUTVA -- stable unit treatment variable assumption.  This means that the potential outcome for patient $u$ is independent of the potential outcome for patient $v$.  We are making this assumption in the DAG based models as well, otherwise explicitly model other patients as nodes.  This is the kind of assumption that gets violated with social network interference.
* Notice that the causal effect we are interested is defined in terms of a specific patient, it is the **unit-level causal effect**: $\text{CE} = B_{u}^{T = 1} - B_{u}^{T = 0}$.  Averaging this over all the patients provides the same average causal effect we have been reasoning about previously.
* So to estimate the average cause effect as follows $$ \sum_{u \in U_1} B_{u}^{T = 1} - B_{u}^{T = 0} \sum_{u \in U_0} $$
* A key difference with our SCM model is that $B_{u}^{T = t}$ is not treated as modeled probabilistically.  They are hidden not random.  In frequentist statistics termonology, they are parameters, not random variables.  The only random variable is in the estimate of the random treatment effect.

## Relationship between SCMs and Potential Outcomes
* Our SCM approach represents potential outcomes using the language of counterfactuals.
* In our SCM model, each patient will have a specific value for $N_B$ and $N_T$.
* $B_u^{T = \tilde{t}} = B^{M: N = n_u; do(T =t)}$
* The i.i.d modeling assumption when we use when we condition on data satifies SUTVA.
* There are theoretical conditions that show when these two representations are equivalent (not discussed here).  Under this equivalency, any theorem that is true in one representation will be true in the other.  That said, some might be easier to prove in one than in the other.
* Potential outcomes also often make an assumption called ignorability.  This means B_{u}^{T = 1} \perp B_{u}^{T = 0} | Z$ where Z is a set of measured covariates.  It is a neccesary assumption for potential outcome tools to be applied.  However, it is not testible.  In the DAG representation, it can be shown that ignoribility holds if Z is a valid adjustment set and blocks all backdoor paths form B to T.  This of course is testible.  This is not to say that the SCM approach is superior, rather, this is an advantage of specifying the assumptions of the model in the form of a DAG.

## Counterfactual probability of neccessary and sufficient cause
* Probability of neccessity: $P^{M; do(X = 0), X = 1, Y = 1}(Y = 0)$ or $P(Y^{do(X = 0)}= 0 | X = 1, Y = 1)$
* Probability of sufficiency: $P^(M: do(X = y), X = 0, Y = 0)(Y = 1)$ = $P(Y^{do(X = 1)}= 1 | X = 0, Y = 0)$
### Examples: 
  * Not neccessary for the soldier to fire for prisoner to die but sufficent. PS = 1, PN = 0
  * Sufficient for firing of the gun to cause person to run under piano.  PS is small. PN = 1
  * Neccessary to have oxygen to burn down the house by not sufficient.
* Traditional (non-counterfactual) statistical machine learning models cannot destiguish between the two using data alone.
* Probability of neccessity and sufficiency: $P(Y^{do(X = 1)} = 1, Y^{do(X = 0)} = 0)$.
* If have monotonicity assumption: changing X from 0 to 1 will never cause Y to change from Y to 0.  Not that this is not the case with the blindness problem.  If we change T from 0 to 1, for that rare population of people blindness will change from 1 to 0.
* We will generalize this property to non-binary distributions later in the class.
* If monotonicty holds, then $PNS = P(Y = 1 | do(X = 1)) - P(Y = 1 | do(X = 0))$




## Mediation

###  Motivating example
* Consider the model:
```{r, mediation, echo = FALSE, message=FALSE, warning=FALSE}
library(bnlearn)
dag <- model2network('[gender][state][department|gender:state][accepted|department:state:gender]')
graphviz.plot(dag)
```
* This is first case where we want to control for a mediator
* Use "do" to hold things constant
* $CDE = P(Y = y |do(X = x), do(M = m)) - P(Y = y |do(X = x), do(M = m))$
* Natural direct effect is defined in terms of counterfactuals: randomize gender, and ask them to apply to the department they would have prefered
* $NDE = P(Y_{M = m} = 1|do(X = 1)) - P(Y_{M = m} = 1|do(X = 0))$
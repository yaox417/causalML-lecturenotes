[
["index.html", "Lecture Notes for Causality in Machine Learning 1 About", " Lecture Notes for Causality in Machine Learning Robert Ness 6/7/2019 1 About Northeastern University Khoury College CS 7290: Summer 2019 These are lecture notes for an ongoing course on causal inference and modeling in machine learning, taught by Dr. Robert O. Ness. These notes are a work in progress, created as the course progresses. They are created by the instructor, the course TA’s Kaushal Paneri and Sicheng Hao, and the Summer 2019 students of this course. "],
["causal-inference-overview-and-course-goals.html", "2 Causal inference overview and course goals 2.1 Course thesis 2.2 Causal modeling as an extension of generative modeling 2.3 Case studies 2.4 Don’t worry about being wrong", " 2 Causal inference overview and course goals Lecture notes prepared by Kaushal Paneiri 2.1 Course thesis The goal of this course is to learn techniques of causal inference in a way that builds on students’ existing intuition and experience with generative machine learning. Moreover, we will do so using frameworks from generative machine learning, include tools for building deep neural networks. Further, when reasoning about causal inference problems, we will bias the case studies to those seen in professional environments where data scientists and machine learning engineers build and manage in-product machine learning models. 2.1.1 Causal modeling as generative ML More specifically, this course focuses on machine learning in the following two ways. We will place causal inference firmly on a foundation of model-based generative machine learning. Our goal is to build machine learning systems that think in causal terms, such as confounding, interventions, and counterfactuals. If you peruse the causal inference literature, you will see examples similar to the A/B test example from epidemiology, econometrics, and clinical trials. This course focuses on the kinds of cases data scientists experience in professional settings, particularly in the tech industry. The focus of the tech industry is shifting towards problems where A/B test becomes more complicated and not feasible. We will cover some advanced techniques like how to deal with confounding, how to build up an online and offline learning and policy evaluation for Markov decision processes that automates testing. We will also cover a little bit of relevant literature from Game theory (Auction models) and Reinforcement learning (policy evaluation and improvement) at the end. 2.1.2 What is left out Causal inference spans many other concepts, and we won’t be able to cover all of them. Though the concepts below are essential, they are out of scope for this course. Causal discovery Causal inference with regression models and various canonical SCM models Doubly-robust estimation Interference due to network effects (important in social network tech companies like Facebook or Twitter) heterogeneous treatment effects deep architectures for causal effect inference causal time series models algorithmic information theory approaches to causal inference 2.1.3 Examples of problems in causal inference To properly contextualize our motivation, we start by understanding how causal inference developed as a field across domains, including economics, biology, social science, computer science, anthropology, epidemiology, statistics. 2.1.3.1 Estimation of causal effects The problem of finding causal effects is the primary motivation of researchers in these domains. For example, in the late 80s and 90s, doctors used to prescribe Hormonal replacement therapy to old women. Experts believed that at the lower age, women have a lower risk of heart disease than men do, but as they age, after menopause, their estrogen level decline. However, after doing a large randomized trial, where women were selected randomly and given either a placebo or estrogen, the results showed that taking estrogen increases the chance of getting heart disease. Causal inference techniques are essential because the stakes are quite high. 2.1.3.2 Counterfactual reasoning with statistics Counterfactual reasoning means observing reality, and then imagining how reality would have unfolded differently had some causal factor been different. For example, “had I broken up with my girlfriend sooner, I would be much happier today” or “had I studied harder for my SATs, I would be in a much better school today.” An example of a question from an experimental context would be “This subject took the drug, and their condition improved. What is the difference between this amount improvement and the improvement they would have seen had they taken placebo?” Counterfactual reasoning is fundamental to how we as humans reason. However, statistical methods are generally not equipped to enable this type of logic. Your counterfactual reasoning process works with data both from actual and hypothetical realities, while your statistical procedure only has access to data from actual reality. The same is true of cutting-edge machine learning. Intuition tells us that if we trained the most powerful deep learning methods to provide us with relationship advice based on our romantic successes and failers, something would be lacking in that advice since those counterfactual outcomes are missing from the training data. 2.1.3.3 The challenge of running experiments In traditional statistics, randomized experiments are the gold standard for discovering the causal effect. An example of a randomized experiment is an A/B test on a new feature in an app. We randomly assign users to two groups and let one group use the feature while the other is presented with a control comparison. We then observe some key outcome, such as conversions. As we will learn, the randomization enables us to conclude the difference between the two groups is the causal effect of the feature on the conversions, because it isolates that effect from other unknown factors that are also affecting the conversions. However, in many instances, setting up this randomization might be complicated. What if users object to not getting a feature that other users are enjoying? What if the experience of the feature and probability of conversion both depend on user-related factors, such that it is unclear how to do proper randomization? What if some users object to being the subjects of an experiment? What if it is unethical to do the experiment? 2.2 Causal modeling as an extension of generative modeling 2.2.1 Generative vs. discriminative Models Let’s focus on supervised learning for a moment. Given a target variable Y and predictor(s) X, a discriminative model learns as much about \\(P(Y| X)\\) as it needs to an optimal prediction. In contrast, generative models try to fully learn the joint distribution \\(P(X, Y)\\) underlying the data. We will discuss this more in later lectures. In simple words, these models can generate data that looks like real data. We focus on generative models because they allow us to build our theories about the data-generating process into the model itself. We will see that we naturally think of this process in causal terms. 2.2.2 Model-based ML and learning to think about the data-generating process The following is the typical checklist in training a statistical machine learning model. Split the data into training and test sets. Choose a few models from literally thousands of algorithm choices. Typically this choice is limited algorithms you are familiar with, are in vogue, or happen to be implemented in the software you have available. Manipulate the data until it fits your algorithm inputs and outputs. Evaluate the model on test data, compare to other models ( optional) If data doesn’t fit the algorithms modeling assumptions, manipulate the data until it does. (optional) If using a deep learning algorithm, search for hyperparameter settings that further optimize prediction. This process works well. However, in this workflow, the data scientist’s time is devoted to manipulating data, hyperparameters, and often, the problem definition itself until things work. An alternative approach is to think hard about the process that generated the data, and then explicitly building your assumptions about that process into a bespoke solution tailored to each new problem. This approach is model-based machine learning. Proponents like this approach because with an excellent model-based machine learning framework, you can create a bespoke solution to pretty much any problem, and don’t need to learn a vast number of machine learning algorithms and techniques. Most interestingly, with the model-based machine learning approach the data scientists shifts her time from transforming her problem to fit some standard algorithm, to thinking hard about the process that generated the problem data, and then building those assumptions into the designing of the algorithm. We’ll see in this class that when we think about the data-generating process, we think about it causally, meaning it has some ordering of causes and effects. In this class, we formalize this intuition by apply causal inference theory to model-based machine learning. 2.2.3 Note on reinforcement learning As reinforcement learning gains in popularity amongst machine learning researchers and practitioners, many may have encountered the term “model-based” for the first time in a reinforcement learning (RL) context. Model-based RL is indeed an example of model-based machine learning. Model-free RL. The agent has no model of the generating process of the data it perceives in the environment; i.e., how states and actions lead to new states and rewards. It can only learn in a Pavlovian sense, relying solely upon experience. Mode-based RL: The agent has a model of the generating process of the data it perceives in the environment. This model enables the agent to make use not only of experience but also of model-based predictions of the consequences of particular actions it has less experience performing. 2.3 Case studies 2.3.1 From linear regression to model-based machine learning The standard Gaussian linear regression model is represented as follows: \\[\\begin{align} \\epsilon &amp;\\sim \\mathcal{N}(0,1)\\\\ Y &amp;= \\beta X + \\alpha + \\epsilon \\end{align}\\] When we read this model specification, it is natural to think of it as predictors \\(X\\) generating target variable \\(Y\\). Indeed, the term generates feels a lot like causes here. Usually, we moderate this feeling by remembering that linear regression models only correlation, and we could just have easily regressed \\(X\\) on \\(Y\\). In this course, we learn how to formalize that feeling. We can turn this model into a generative model by placing a marginal distribution on X. \\[\\begin{align} \\epsilon &amp;\\sim \\mathcal{N}(0,1)\\nonumber\\\\ X &amp;\\sim P_X\\nonumber \\\\ Y &amp;= \\beta X + \\alpha + \\epsilon \\nonumber \\end{align}\\] At this point, we are already telling a data generating story where \\(Y\\) comes from \\(X\\) and \\(\\epsilon\\). Now let’s expand on that story. Suppose we observe that \\(Y\\) is measured from some instrument, and we suppose that this instrument is adding technical noise to \\(Y\\). Now the regression model becomes a noise model. \\[\\begin{align} \\epsilon &amp;\\sim \\mathcal{N}(0,1)\\nonumber\\\\ X &amp;\\sim P_X\\nonumber \\\\ Z &amp;\\sim P_Z \\nonumber\\\\ Y &amp;= \\beta X + \\alpha + \\epsilon + Z \\nonumber \\end{align}\\] 2.3.2 Binary classifier The logistic regression model has the form: \\[\\mathbb{E}[Y] = \\texttt{logit}(\\beta X + \\alpha)\\] If we read this formula, it reads as Y comes from X. Of course that is not true, this model doesn’t care whether Y comes from X or vice versa, in other words, it doesn’t care how Y is generated, it merely wants to model \\(P(Y=1|X)\\) In contrast, a naive Bayes classifier models P(X, Y) as P(X|Y)P(Y). P(X|Y) and P(Y) are estimated from the data, and then we use Bayes rule to find P(Y|X) and predict Y given X. P(X|Y)P(Y) is a representation of the data generating process that reads as “there is some unobserved Y, and then we observe X which came from Y.” There is nothing that forces us to apply naive Bayes only in problems where the generation of the prediction target generation of the features. Yet, this is precisely the kind of problem where this approach tends to get applied, such as spam detection. I argue that it P(X|Y)P(Y) aligns with a causal intuition that X comes from Y, and we avoid the inner cringe that comes from using naive Bayes when we suspect that Y comes from X. Causal modeling gives us a language to formalize this intuition. 2.3.3 Gaussian mixture model The naive Bayes classifier is an example of a latent variable model. Latent variable models come with a pre-packaged data-generating story. Another example is the Gaussian mixture model. Image Let’s recall the intuition with simple GMM with two Gaussians. We observe the data and realize that it is coming from two Gaussians with means \\(\\mu_1\\), and \\(\\mu_2\\). Let \\(Z_i\\) be a binary variable that says \\(X_i\\) belongs to one of these distributions. The data generating process is: 1. Some probabilistic process generated \\(\\mu\\). 2. Some Dirichlet distribution generated \\(\\theta\\). 3. Then for each \\(i\\) in some range 1. a discrete distribution with parameter \\(\\theta\\) generated \\(Z_i\\). 2. \\(Z_i\\) picks a \\(\\mu\\) that generates \\(X_i\\) from a Gaussian with mean \\(\\mu\\). We can represent this data generative process in code quite easily. The following pseudocode generalizes from two to k mixture components. ``` function (alphas, sigma, scale): theta = random_dirichlet(alphas)) for each mixture component k: mu[k] = random_normal(0, sigma)) for each data_point: Z[i] = random_discrete(theta)) X[i] = random_normal(mu[Z[i]], scale)) ``` Now inferring the mixture components given data using this code requires an inference algorithm. Model-based machine learning frameworks generally let you code up the model just as above and then provide implementations of algorithms that can provide inference on the model. In the next lecture, we will cover the basics of two frameworks for model-based machine learning that implement inference algorithms. The GMM and other latent variable models like the hidden Markov model, mixed membership models like LDA, linear factor models, provide an off-the-shelf data-generating story that is straightforward to cast into code. However, just as we turned regression into a noise model, we can adjust the model and code to create a bespoke solution to a unique problem. 2.3.4 Deep generative models Deep generative models are generative models that use deep neural network architectures. Examples include variational autoencoders and generative adversarial networks. Rather than make the data generation story explicit, their basic implementation compresses all generative process into a latent encoding. But nothing is forcing them to do so. In this course, we will see examples of deep generative models where we model the critical components of the data generating process explicitly, and let the latent encoding handle nuisance variables that we don’t care about. 2.4 Don’t worry about being wrong Deep learning works well because they essentially infer the optimal circuit between a given input signal and output channels. In contrast, when you reason about and represent as code the data generating process, you are inferring a program given program inputs and outputs. In machine learning, the task of automatically inferring a program is called program induction, and it is much harder than inferring a circuit. Indeed, that is an ill-specified problem, because there are numerous programs we could write to generate the same data. Algorithmic information theory tells us that the task of finding the shortest program that produces an output from a given input is an NP-hard problem. So if program induction is hard for computers, it should surprise us that it is challenging for humans too. In practice, we make use of domain knowledge and other constraints. For example, an economist building a price model might incorporate in their understanding of supply and demand. A computational biologist may use extensive databases of verified relationships between genes in modeling. Finally, we can still validate the model on the data using goodness-of-fit or predictive performance statistics. We can also use standard techniques for handling uncertainty in our models, such as ensemble methods. "],
["tutorial-probabilistic-modeling-with-bayesian-networks-and-bnlearn.html", "3 Tutorial probabilistic modeling with Bayesian networks and bnlearn 3.1 Installing bnlearn 3.2 Understanding the directed acyclic graph representation 3.3 Estimating parameters of conditional probability tables 3.4 Conditional independence in Bayesian networks 3.5 Plotting Conditional Probability Distributions", " 3 Tutorial probabilistic modeling with Bayesian networks and bnlearn Lecture notes by Sara Taheri 3.1 Installing bnlearn Open RStudio and in console type: install.packages(bnlearn) install.packages(Rgraphviz) If you experience problems installing Rgraphviz, try the following script: if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;Rgraphviz&quot;) Then type “bnlearn” in the window that appears and click on the install button. Do the same thing for the other package. 3.2 Understanding the directed acyclic graph representation In this part, we introduce survey data set and show how we can visualize it with bnlearn package. 3.2.1 The survey data dataset Survey data is a data set that contains information about usage of different transportation systems with a focus on cars and trains for different social groups. It includes these factors: Age (A): It is recorded as young (young) for individuals below 30 years, adult (adult) for individuals between 30 and 60 years old, and old (old) for people older than 60. Sex (S): The biological sex of individual, recorded as male (M) or female (F). Education (E): The highest level of education or training completed by the individual, recorded either high school (high) or university degree (uni). Occupation (O): It is recorded as an employee (emp) or a self employed (self) worker. Residence (R): The size of the city the individual lives in, recorded as small (small) or big (big). Travel (T): The means of transport favoured by the individual, recorded as car (car), train (train) or other (other) Travel is the target of the survey, the quantity of interest whose behaviour is under investigation. 3.2.2 Visualizing a Bayesian network We can represent the relationships between the variables in the survey data by a directed graph where each node correspond to a variable in data and each edge represents conditional dependencies between pairs of variables. In bnlearn, we can graphically represent the relationships between variables in survey data like this: # empty graph library(bnlearn) ## Warning: package &#39;bnlearn&#39; was built under R version 3.4.4 ## ## Attaching package: &#39;bnlearn&#39; ## The following object is masked from &#39;package:BiocGenerics&#39;: ## ## score ## The following object is masked from &#39;package:stats&#39;: ## ## sigma dag &lt;- empty.graph(nodes = c(&quot;A&quot;,&quot;S&quot;,&quot;E&quot;,&quot;O&quot;,&quot;R&quot;,&quot;T&quot;)) arc.set &lt;- matrix(c(&quot;A&quot;, &quot;E&quot;, &quot;S&quot;, &quot;E&quot;, &quot;E&quot;, &quot;O&quot;, &quot;E&quot;, &quot;R&quot;, &quot;O&quot;, &quot;T&quot;, &quot;R&quot;, &quot;T&quot;), byrow = TRUE, ncol = 2, dimnames = list(NULL, c(&quot;from&quot;, &quot;to&quot;))) arcs(dag) &lt;- arc.set nodes(dag) ## [1] &quot;A&quot; &quot;S&quot; &quot;E&quot; &quot;O&quot; &quot;R&quot; &quot;T&quot; arcs(dag) ## from to ## [1,] &quot;A&quot; &quot;E&quot; ## [2,] &quot;S&quot; &quot;E&quot; ## [3,] &quot;E&quot; &quot;O&quot; ## [4,] &quot;E&quot; &quot;R&quot; ## [5,] &quot;O&quot; &quot;T&quot; ## [6,] &quot;R&quot; &quot;T&quot; 3.2.3 Plotting the DAG In this section we discuss the ways that we can visually demonstrate Bayesian networks. You can either use the simple plot function or use the graphviz.plot function from Rgraphviz package. # plot dag with plot function plot(dag) # plot dag with graphviz.plot function. Default layout is dot graphviz.plot(dag, layout = &quot;dot&quot;) # plot dag with graphviz.plot function. change layout to &quot;fdp&quot; graphviz.plot(dag, layout = &quot;fdp&quot;) # plot dag with graphviz.plot function. change layout to &quot;circo&quot; graphviz.plot(dag, layout = &quot;circo&quot;) 3.2.4 Highlighting specific nodes If you want to change the color of the nodes or the edges of your graph, you can do this easily by adding a highlight input to the graphviz.plot function. Let’s assume that we want to change the color of all the nodes and edges of our dag to blue. hlight &lt;- list(nodes = nodes(dag), arcs = arcs(dag), col = &quot;blue&quot;, textCol = &quot;blue&quot;) pp &lt;- graphviz.plot(dag, highlight = hlight) The look of the arcs can be customised as follows using the edgeRenderInfo function from Rgraphviz. edgeRenderInfo(pp) &lt;- list(col = c(&quot;S~E&quot; = &quot;black&quot;, &quot;E~R&quot; = &quot;black&quot;), lwd = c(&quot;S~E&quot; = 3, &quot;E~R&quot; = 3)) Attributes being modified (i.e., col for the colour and lwd for the line width) are specified again as the elements of a list. For each attribute, we specify a list containing the arcs we want to modify and the value to use for each of them. Arcs are identified by labels of the form parent∼child, e.g., S → E is S~E. Similarly, we can highlight nodes with nodeRenderInfo. We set their colour and the colour of the node labels to black and their background to grey. nodeRenderInfo(pp) &lt;- list(col = c(&quot;S&quot; = &quot;black&quot;, &quot;E&quot; = &quot;black&quot;, &quot;R&quot; = &quot;black&quot;), fill = c(&quot;E&quot; = &quot;grey&quot;)) Once we have made all the desired modifications, we can plot the DAG again with the renderGraph function from Rgraphviz. renderGraph(pp) 3.2.5 The directed acyclic graph as a representation of joint probability The DAG represents a factorization of the joint probability distribution into a joint probability distribution. In this section we show how to add custom probability distributions to a DAG, as well as how to estimate the parameters of the conditional probability distribution using maximum likelihood estimation or Bayesian estimation. 3.2.6 Specifying the probability distributions on your own Given the DAG, the joint probability distribution of the survey data variables factorizes as follows: \\(Pr(A, S, E, O, R, T) = Pr(A) Pr(S) Pr(E | A, S) Pr(O | E) Pr(R | E) Pr(T | O, R).\\) A.lv &lt;- c(&quot;young&quot;, &quot;adult&quot;, &quot;old&quot;) S.lv &lt;- c(&quot;M&quot;, &quot;F&quot;) E.lv &lt;- c(&quot;high&quot;, &quot;uni&quot;) O.lv &lt;- c(&quot;emp&quot;, &quot;self&quot;) R.lv &lt;- c(&quot;small&quot;, &quot;big&quot;) T.lv &lt;- c(&quot;car&quot;, &quot;train&quot;, &quot;other&quot;) A.prob &lt;- array(c(0.3,0.5,0.2), dim = 3, dimnames = list(A = A.lv)) S.prob &lt;- array(c(0.6,0.4), dim = 2, dimnames = list(S = S.lv)) E.prob &lt;- array(c(0.75,0.25,0.72,0.28,0.88,0.12,0.64,0.36,0.70,0.30,0.90,0.10), dim = c(2,3,2), dimnames = list(E = E.lv, A = A.lv, S = S.lv)) O.prob &lt;- array(c(0.96,0.04,0.92,0.08), dim = c(2,2), dimnames = list(O = O.lv, E = E.lv)) R.prob &lt;- array(c(0.25,0.75,0.2,0.8), dim = c(2,2), dimnames = list(R = R.lv, E = E.lv)) T.prob &lt;- array(c(0.48,0.42,0.10,0.56,0.36,0.08,0.58,0.24,0.18,0.70,0.21,0.09), dim = c(3,2,2), dimnames = list(T = T.lv, O = O.lv, R = R.lv)) cpt &lt;- list(A = A.prob, S = S.prob, E = E.prob, O = O.prob, R = R.prob, T = T.prob) # custom cpt table cpt ## $A ## A ## young adult old ## 0.3 0.5 0.2 ## ## $S ## S ## M F ## 0.6 0.4 ## ## $E ## , , S = M ## ## A ## E young adult old ## high 0.75 0.72 0.88 ## uni 0.25 0.28 0.12 ## ## , , S = F ## ## A ## E young adult old ## high 0.64 0.7 0.9 ## uni 0.36 0.3 0.1 ## ## ## $O ## E ## O high uni ## emp 0.96 0.92 ## self 0.04 0.08 ## ## $R ## E ## R high uni ## small 0.25 0.2 ## big 0.75 0.8 ## ## $T ## , , R = small ## ## O ## T emp self ## car 0.48 0.56 ## train 0.42 0.36 ## other 0.10 0.08 ## ## , , R = big ## ## O ## T emp self ## car 0.58 0.70 ## train 0.24 0.21 ## other 0.18 0.09 Now that we have defined both the DAG and the local distribution corresponding to each variable, we can combine them to form a fully-specified BN. We combine the DAG we stored in dag and a list containing the local distributions, which we will call cpt, into an object of class bn.fit called bn. # fit cpt table to network bn &lt;- custom.fit(dag, cpt) 3.3 Estimating parameters of conditional probability tables For the hypothetical survey described in this chapter, we have assumed to know both the DAG and the parameters of the local distributions defining the BN. In this scenario, BNs are used as expert systems, because they formalise the knowledge possessed by one or more experts in the relevant fields. However, in most cases the parameters of the local distributions will be estimated (or learned) from an observed sample. Let’s read the survey data: survey &lt;- read.table(&quot;data/survey.txt&quot;, header = TRUE) head(survey) ## A R E O S T ## 1 adult big high emp F car ## 2 adult small uni emp M car ## 3 adult big uni emp F train ## 4 adult big high emp M car ## 5 adult big high emp M car ## 6 adult small high emp F train In the case of this survey, and of discrete BNs in general, the parameters to estimate are the conditional probabilities in the local distributions. They can be estimated, for example, with the corresponding empirical frequencies in the data set, e.g., \\(\\hat{Pr}(O = emp | E = high) = \\frac{\\hat{Pr}(O = emp, E = high)}{\\hat{Pr}(E = high)}= \\frac{\\text{number of observations for which O = emp and E = high}}{\\text{number of observations for which E = high}}\\) This yields the classic frequentist and maximum likelihood estimates. In bnlearn, we can compute them with the bn.fit function. bn.fit complements the custom.fit function we used in the previous section; the latter constructs a BN using a set of custom parameters specified by the user, while the former estimates the same from the data. bn.mle &lt;- bn.fit(dag, data = survey, method = &quot;mle&quot;) bn.mle ## ## Bayesian network parameters ## ## Parameters of node A (multinomial distribution) ## ## Conditional probability table: ## adult old young ## 0.472 0.208 0.320 ## ## Parameters of node S (multinomial distribution) ## ## Conditional probability table: ## F M ## 0.402 0.598 ## ## Parameters of node E (multinomial distribution) ## ## Conditional probability table: ## ## , , S = F ## ## A ## E adult old young ## high 0.6391753 0.8461538 0.5384615 ## uni 0.3608247 0.1538462 0.4615385 ## ## , , S = M ## ## A ## E adult old young ## high 0.7194245 0.8923077 0.8105263 ## uni 0.2805755 0.1076923 0.1894737 ## ## ## Parameters of node O (multinomial distribution) ## ## Conditional probability table: ## ## E ## O high uni ## emp 0.98082192 0.92592593 ## self 0.01917808 0.07407407 ## ## Parameters of node R (multinomial distribution) ## ## Conditional probability table: ## ## E ## R high uni ## big 0.7178082 0.8666667 ## small 0.2821918 0.1333333 ## ## Parameters of node T (multinomial distribution) ## ## Conditional probability table: ## ## , , R = big ## ## O ## T emp self ## car 0.58469945 0.69230769 ## other 0.19945355 0.15384615 ## train 0.21584699 0.15384615 ## ## , , R = small ## ## O ## T emp self ## car 0.54700855 0.75000000 ## other 0.07692308 0.25000000 ## train 0.37606838 0.00000000 Note that we assume we know the structure of the network, so dag is an input of bn.fit function. As an alternative, we can also estimate the same conditional probabilities in a Bayesian setting, using their posterior distributions. In this case, the method argument of bn.fit must be set to “bayes”. bn.bayes &lt;- bn.fit(dag, data = survey, method = &quot;bayes&quot;, iss = 10) The estimated posterior probabilities are computed from a uniform prior over each conditional probability table. The iss optional argument, whose name stands for imaginary sample size (also known as equivalent sample size), determines how much weight is assigned to the prior distribution compared to the data when computing the posterior. The weight is specified as the size of an imaginary sample supporting the prior distribution. 3.3.1 Fit dag to data and predict the value of latent variable # predicting a variable in the test set. training = bn.fit(model2network(&quot;[A][B][E][G][C|A:B][D|B][F|A:D:E:G]&quot;), gaussian.test[1:2000, ]) test = gaussian.test[2001:nrow(gaussian.test), ] predicted &lt;- predict(training, node = &quot;A&quot;, data = test, method = &quot;bayes-lw&quot;) head(predicted) ## [1] 3.3122356 0.5913880 0.6231941 1.7209945 1.0202067 1.0317637 about the method bayes-lw: the predicted values are computed by averaging likelihood weighting simulations performed using all the available nodes as evidence (obviously, with the exception of the node whose values we are predicting). If the variable being predicted is discrete, the predicted level is that with the highest conditional probability. If the variable is continuous, the predicted value is the expected value of the conditional distribution. 3.4 Conditional independence in Bayesian networks Using a DAG structure we can investigate whether a variable is conditionally independent from another variable given a set of variables from the DAG. If the variables depend directly on each other, there will be a single arc connecting the nodes corresponding to those two variables. If the dependence is indirect, there will be two or more arcs passing through the nodes that mediate the association. If \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are separated by \\(\\textbf{Z}\\), we say that \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are conditionally independent given \\(\\textbf{Z}\\) and denote it with, \\[\\textbf{X } { \\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{Y } | \\textbf{ Z}\\] Conditioning on \\(\\textbf{Z}\\) is equivalent to fixing the values of its elements, so that they are known quantities. \\(\\textbf{Definition (MAPS).}\\) Let M be the dependence structure of the probability distribution P of data \\(\\textbf{D}\\), that is, the set of conditional independence relationships linking any triplet \\(\\textbf{X}\\), \\(\\textbf{Y}\\), \\(\\textbf{Z}\\) of subsets of \\(\\textbf{D}\\). A graph G is a dependency map (or D-map) of M if there is a one-to-one correspondence between the random variables in \\(\\textbf{D}\\) and the nodes \\(\\textbf{V}\\) of G such that for all disjoint subsets \\(\\textbf{X}\\), \\(\\textbf{Y}\\), \\(\\textbf{Z}\\) of \\(\\textbf{D}\\) we have \\[\\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{P} \\textbf{ Y } | \\textbf{ Z} \\Longrightarrow \\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{ Y } | \\textbf{ Z}\\] Similarly, G is an independency map (or I-map) of M if \\[\\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{P} \\textbf{ Y } | \\textbf{ Z} \\Longleftarrow \\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{ Y } | \\textbf{ Z}\\] G is said to be a perfect map of M if it is both a D-map and an I-map, that is \\[\\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{P} \\textbf{ Y } | \\textbf{ Z} \\Longleftrightarrow \\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{ Y } | \\textbf{ Z}\\] and in this case G is said to be faithful or isomorphic to M. \\(\\textbf{Definition.}\\) A variable V is a collider or has a V structure, if it has 2 upcoming parents. (#fig:02_colider_net)V is a collider You can find all the V structures of a DAG: vstructs(dag) ## X Z Y ## [1,] &quot;A&quot; &quot;E&quot; &quot;S&quot; ## [2,] &quot;O&quot; &quot;T&quot; &quot;R&quot; Note that conditioning on a collider induces dependence even though the parents aren’t directly connected. \\(\\textbf{Definition (d-separation)}\\) If G is a directed graph in which \\(\\textbf{X}\\), \\(\\textbf{Y}\\) and \\(\\textbf{Z}\\) are disjoint sets of vertices, then \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are d-connected by \\(\\textbf{Z}\\) in G if and only if there exists an undirected path U between some vertex in \\(\\textbf{X}\\) and some vertex in \\(\\textbf{Y}\\) such that for every collider C on U, either C or a descendent of C is in \\(\\textbf{Z}\\), and no non-collider on U is in \\(\\textbf{Z}\\). \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are d-separated by \\(\\textbf{Z}\\) in G if and only if they are not d-connected by \\(\\textbf{Z}\\) in G. We assume that graphical separation (\\({\\!\\perp\\!\\!\\!\\perp}_{G}\\)) implies probabilistic independence (\\({\\!\\perp\\!\\!\\!\\perp}_{P}\\)) in a Bayesian network. We can investigate whether two nodes in a bn object are d-separated using the dsep function. dsep takes three arguments, x, y and z, corresponding to \\(\\textbf{X}\\), \\(\\textbf{Y}\\) and \\(\\textbf{Z}\\); the first two must be the names of two nodes being tested for d-separation, while the latter is an optional d-separating set. So, for example, dsep(dag, x = &quot;S&quot;, y = &quot;R&quot;) ## [1] FALSE dsep(dag, x = &quot;O&quot;, y = &quot;R&quot;) ## [1] FALSE dsep(dag, x = &quot;S&quot;, y = &quot;R&quot;, z = &quot;E&quot;) ## [1] TRUE 3.4.1 Markov Property, Equivalence classes and CPDAGS \\(\\textbf{Definition (Local Markov property)}\\) Each node \\(X_i\\) is conditionally independent of its non-descendants given its parents. Compared to the previous decomposition, it highlights the fact that parents are not completely independent from their children in the BN; a trivial application of Bayes’ theorem to invert the direction of the conditioning shows how information on a child can change the distribution of the parent. Second, assuming the DAG is an I-map also means that serial and divergent connections result in equivalent factorisations of the variables involved. It is easy to show that \\[\\begin{align} Pr(X_i) Pr(X_j | X_i) Pr(X_k | X_j) &amp;= Pr(X_j,X_i) Pr(X_k | X_j)\\\\ &amp;= Pr(X_i | X_j) Pr(X_j) Pr(X_k | X_j) \\end{align}\\] Then \\(X_i \\longrightarrow X_j \\longrightarrow X_k\\) and \\(X_i \\longleftarrow X_j \\longrightarrow X_k\\) are equivalent. As a result, we can have BNs with different arc sets that encode the same conditional independence relationships and represent the same global distribution in different (but probabilistically equivalent) ways. Such DAGs are said to belong to the same equivalence class. 3.4.2 Skeleton of a network, CPDAGs and equivalence classes The skeleton of a network is the network without any direction. Here is the skeleton of the dag for survey dataset. graphviz.plot(skeleton(dag)) \\(\\textbf{Theorem. (Equivalence classes)}\\) Two DAGs defined over the same set of variables are equivalent and only they have the same skeleton (i.e., the same underlying undirected graph) and the same v-structures. data(learning.test) learn.net1 = empty.graph(names(learning.test)) learn.net2 = empty.graph(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;)) modelstring(learn.net1) = &quot;[A][C][F][B|A][D|A:C][E|B:F]&quot; arc.set2 &lt;- matrix(c(&quot;B&quot;, &quot;A&quot;, &quot;A&quot;, &quot;D&quot;, &quot;C&quot;, &quot;D&quot;, &quot;B&quot;, &quot;E&quot;, &quot;F&quot;, &quot;E&quot;), byrow = TRUE, ncol = 2, dimnames = list(NULL, c(&quot;from&quot;, &quot;to&quot;))) arcs(learn.net2) &lt;- arc.set2 graphviz.compare(learn.net1,learn.net2) score(learn.net1, learning.test, type = &quot;loglik&quot;) ## [1] -23832.13 score(learn.net2, learning.test, type = &quot;loglik&quot;) ## [1] -23832.13 # type == &quot;loglik&quot; means you get the log likelihood of the data given the dag and the MLE of the parameters In other words, the only arcs whose directions are important are those that are part of one or more v-structures. The skeleton of a DAG and it’s V structures identify the equivalence class the DAG belongs to, which is represented by the completed partially directed graph (CPDAG). We can obtain it from a DAG with cpdag function. X &lt;- paste(&quot;[X1][X3][X5][X6|X8][X2|X1][X7|X5][X4|X1:X2]&quot;, &quot;[X8|X3:X7][X9|X2:X7][X10|X1:X9]&quot;, sep = &quot;&quot;) dag2 &lt;- model2network(X) par(mfrow = c(1,2)) graphviz.plot(dag2) graphviz.plot(cpdag(dag2)) 3.4.3 Moral Graphs In previous Section we introduced an alternative graphical representation of the DAG underlying a BN: the CPDAG of the equivalence class the BN belongs to. Another graphical representation that can be derived from the DAG is the moral graph. The moral graph is an undirected graph that is constructed as follows: connecting the non-adjacent nodes in each v-structure with an undirected arc; ignoring the direction of the other arcs, effectively replacing them with undirected arcs. This transformation is called moralisation because it “marries” non-adjacent parents sharing a common child. In the case of our example dag, we can create the moral graph with the moral function as follows: graphviz.plot(moral(dag2)) Moralisation has several uses. First, it provides a simple way to transform a BN into the corresponding Markov network, a graphical model using undirected graphs instead of DAGs to represent dependencies. In a Markov network, we say that \\(\\textbf{X} {\\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{Y} | \\textbf{Z}\\) if every path between \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) contains some node \\(Z \\in \\textbf{Z}\\). 3.5 Plotting Conditional Probability Distributions Plotting the conditional probabilities associated with a conditional probability table or a query is also useful for diagnostic and exploratory purposes. Such plots can be difficult to read when a large number of conditioning variables is involved, but nevertheless they provide useful insights for most synthetic and real-world data sets. As far as conditional probability tables are concerned, bnlearn provides functions to plot barcharts (bn.fit.barchart) and dot plots (bn.fit.dotplot) from bn.fit objects. Both functions are based on the lattice package. For example let’s look at the conditional plot of \\(Pr(T | R,O)\\): bn.fit.barchart(bn.mle$T, main = &quot;Travel&quot;, xlab = &quot;Pr(T | R,O)&quot;, ylab = &quot;&quot;) ## Loading required namespace: lattice ## Warning: package &#39;reticulate&#39; was built under R version 3.4.4 "],
["tutorial-on-deep-probabilitic-modeling-with-pyro.html", "4 Tutorial on deep probabilitic modeling with Pyro 4.1 Recap on Motivation 4.2 Introduction to Pyro 4.3 Inference 4.4 Some other Pyro vocabulary", " 4 Tutorial on deep probabilitic modeling with Pyro import torch import pyro pyro.set_rng_seed(101) 4.1 Recap on Motivation Our goal is to understand causal modeling within the context of generative machine learning. We just examined one generative machine learning framework called Bayesian networks (BNs) and how we can use BNs as causal models. Bayesian Networks (BNs) Framework that defines a probabilistic generative model of the world in terms of a directed acyclic graph. causal Bayesian networks: Bayesian networks where the direction of edges in the DAG represent causality. Bayesian networks provide a general-purpose framework for representing a causal data generating story for how the world works. Now we will introduce probabilistic programming, a framework that is more expressive than Bayesian networks. 4.1.1 What is a probabilistic programming language? “A probabilistic programming language (PPL) is a programming language designed to describe probabilistic models and then perform inference in those models. PPLs are closely related to graphical models and Bayesian networks but are more expressive and flexible. Probabilistic programming represents an attempt to”Unify general purpose programming&quot; with probabilistic modeling.&quot; -Wikipedia A PPL is a domain-specific programming language for that lets you write a data generating story as a program. As with a causal Bayesian network, you can write your program in a way that orders the steps of its execution according to cause and effect. 4.1.2 How exactly do Bayesian networks and probabilistic programming differ? Representation of relationships between variables. BNs restricted to representing the relationships between variables in terms of conditional probability distributions (CPDs) factored according to a DAG. Frameworks typically limit you to a small set of parametric CPDs (e.g., Gaussian, multinomial). Just as computer programs are more expressive than flow charts, PPLs let you represent relations any way you like so long as you can represent them in code. PPL relationships can include control flow and recursion. In causal models, we will see that this allows you to be more specific about mechanism than you can with CPDs. DAG vs. open world models. BNs restrict the representation of the joint distribution to a DAG. This constraint enables you to reason easily about the joint distribution through graph-theoretic operations like d-separation. PPLs need not be constrained to a DAG. For example (using an imaginary Python PPL package): X = Bernoulli(p) if X == 1: Y = Gaussian(0, 1) In a DAG, you have a fixed set of variables, i.e. a “closed world”. In the above model, the variable Y is only instantiated if X==1. Y may or may not exist depending on how the generative process unfolds. For a more extreme example, consider this: X = Poisson(λ) Y = [Gaussian(0, 1)] for i in range(1, X): Y[i] = Gaussian(Y[i-1], 1)) Here you have the total number of Y variables itself being a random variable X. Further, the mean of the ith Y is a random variable given by the i-1th Y. You can’t do that with a Bayes net! Unfortunately, we can’t reason about this as directly as we can with a DAG. For example, recall that with the DAG, we had a convenient algorithm called CPDAG that converts the DAG to a partially directed acyclic graph structure called a PDAG that provides a compact representation of all the DAGs in an equivalence class. How might we define an equivalence class on this program? Certainly, enumerating all programs with an equivalent representation of the joint distribution would be very difficult even with constraints on the length of the program. In general, enumerating all programs of minimal description that provide equivalent representations of a joint distribution is an NP-hard problem. Inference When you have a DAG and a constrained set of parametric CPDs, as well as constraints on the kind of inference, queries the user can make, you can implement some inference algorithms in your BN framework that will generally work in a reasonable amount of time. PPLs are more flexible than BNs, but the trade-off s that getting inference to work is harder. PPL’s develop several abstractions for inference and leave it to the user to apply them, requiring the user to become something of an expert in inference algorithms. PPL developers make design decisions to make inference easier for the user, though this often sacrifices some flexibility. One emergent pattern is to build PPLs on tensor-based frameworks like Tensorflow and PyTorch. Tensor-based PPLs allow a data scientist with experience building deep learning models to rely on that experience when doing inference. Image \\[\\texttt{Kevin Smith - Tutorial: Probabilistic Programming}\\] 4.2 Introduction to Pyro Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. Pyro enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling. Our purpose of this class, pyro has “do”-operator that allows intervention and counterfactual inference in these probabilistic models. 4.2.1 Stochastic Functions The basic unit of probabilistic programs is the stochastic function. A stochastic function is an arbitrary Python callable that combines two ingredients: deterministic Python code; and primitive stochastic functions that call a random number generator For this course, we will consider these stochastic functions as models. Stochastic functions can be used to represent simplified or abstract descriptions of a data-generating process. 4.2.2 Primitive stochastic functions We call them distributions. We can explicitly compute the probability of the outputs given the inputs. loc = 0. # mean zero scale = 1. # unit variance normal = torch.distributions.Normal(loc, scale) # create a normal distribution object x = normal.rsample() # draw a sample from N(0,1) print(&quot;sample: &quot;, x) ## sample: tensor(-1.3905) Pyro simplifies this process of sampling from distributions. It uses pyro.sample(). x = pyro.sample(&quot;my_sample&quot;, pyro.distributions.Normal(loc, scale)) print(x) ## tensor(-0.8152) Just like a direct call to torch.distributions.Normal().rsample(), this returns a sample from the unit normal distribution. The crucial difference is that this sample is named. Pyro’s backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. This is how Pyro can implement the various manipulations that underlie inference algorithms. Let’s write a simple weather model. 4.2.3 A simple model import pyro.distributions as dist def weather(): cloudy = pyro.sample(&#39;cloudy&#39;, dist.Bernoulli(0.3)) cloudy = &#39;cloudy&#39; if cloudy.item() == 1.0 else &#39;sunny&#39; mean_temp = {&#39;cloudy&#39;: 55.0, &#39;sunny&#39;: 75.0}[cloudy] scale_temp = {&#39;cloudy&#39;: 10.0, &#39;sunny&#39;: 15.0}[cloudy] temp = pyro.sample(&#39;temp&#39;, dist.Normal(mean_temp, scale_temp)) return cloudy, temp.item() for _ in range(3): print(weather()) ## (&#39;cloudy&#39;, 64.5440444946289) ## (&#39;sunny&#39;, 94.37557983398438) ## (&#39;sunny&#39;, 72.5186767578125) First two lines introduce a binary variable cloudy, which is given by a draw from the Bernoulli distribution with a parameter of \\(0.3\\). The Bernoulli distribution returns either \\(0\\) or \\(1\\), line 2 converts that into a string. So, So according to this model, \\(30%\\) of the time it’s cloudy and \\(70%\\) of the time it’s sunny. In line 4 and 5, we initialize mean and scale of the temperature for both values. We then sample, the temperature from a Normal distribution and return that along with cloudy variable. We can build complex model by modularizing and reusing the concepts into functions and use them as programmers use functions. def ice_cream_sales(): cloudy, temp = weather() expected_sales = 200. if cloudy == &#39;sunny&#39; and temp &gt; 80.0 else 50. ice_cream = pyro.sample(&#39;ice_cream&#39;, pyro.distributions.Normal(expected_sales, 10.0)) return ice_cream 4.3 Inference As we discussed earlier, the reason we use PPLs is because they can easily go backwards and reason about cause given the observed effect. There are myriad of inference algorithms available in pyro. Let’s try it on an even simpler model. \\[weight \\mid guess \\sim \\mathcal{N}(guess, 1)\\] \\[measurement \\mid guess, weight \\sim \\mathcal{N}(weight, 0.75)\\] def scale(guess): weight = pyro.sample(&quot;weight&quot;, dist.Normal(guess, 1.0)) measurement = pyro.sample(&quot;measurement&quot;, dist.Normal(weight, 0.75)) return measurement scale(5.) ## tensor(5.1712) Suppose we observe that the measurement of an object was \\(14\\) lbs. What would have we guessed if we tried to guess it’s weight first? This question is answered in two steps. Condition the model. conditioned_scale = pyro.condition(scale, data={&quot;measurement&quot;: torch.tensor(14.)}) Set the prior and infer the posterior. We will use from pyro.infer.mcmc import MCMC from pyro.infer.mcmc.nuts import HMC from pyro.infer import EmpiricalMarginal import matplotlib.pyplot as plt # %matplotlib inline guess_prior = 10. hmc_kernel = HMC(conditioned_scale, step_size=0.9, num_steps=4) posterior = MCMC(hmc_kernel, num_samples=1000, warmup_steps=50).run(guess_prior) ## Warmup: 0%| | 0/1050 [00:00, ?it/s] Warmup: 0%| | 0/1050 [00:00, ?it/s, step size=6.42e+00, acc. rate=1.000] Warmup: 0%| | 1/1050 [00:00, 69.62it/s, step size=1.08e+00, acc. rate=0.500] Warmup: 0%| | 2/1050 [00:00, 116.43it/s, step size=1.23e-01, acc. rate=0.333] Warmup: 0%| | 3/1050 [00:00, 78.38it/s, step size=1.68e-01, acc. rate=0.500] Warmup: 0%| | 4/1050 [00:00, 74.15it/s, step size=2.65e-01, acc. rate=0.600] Warmup: 0%| | 5/1050 [00:00, 78.08it/s, step size=4.46e-01, acc. rate=0.667] Warmup: 1%| | 6/1050 [00:00, 85.07it/s, step size=8.00e-01, acc. rate=0.714] Warmup: 1%| | 7/1050 [00:00, 94.44it/s, step size=6.91e-01, acc. rate=0.750] Warmup: 1%| | 8/1050 [00:00, 101.98it/s, step size=1.29e+00, acc. rate=0.778] Warmup: 1%| | 9/1050 [00:00, 111.52it/s, step size=1.02e-01, acc. rate=0.700] Warmup: 1%| | 10/1050 [00:00, 93.47it/s, step size=1.93e-01, acc. rate=0.727] Warmup: 1%|1 | 11/1050 [00:00, 102.76it/s, step size=1.93e-01, acc. rate=0.727] Warmup: 1%|1 | 11/1050 [00:00, 102.76it/s, step size=3.66e-01, acc. rate=0.750] Warmup: 1%|1 | 12/1050 [00:00, 102.76it/s, step size=6.89e-01, acc. rate=0.769] Warmup: 1%|1 | 13/1050 [00:00, 102.76it/s, step size=1.22e+00, acc. rate=0.786] Warmup: 1%|1 | 14/1050 [00:00, 102.76it/s, step size=1.03e-01, acc. rate=0.733] Warmup: 1%|1 | 15/1050 [00:00, 102.76it/s, step size=1.96e-01, acc. rate=0.750] Warmup: 2%|1 | 16/1050 [00:00, 102.76it/s, step size=3.57e-01, acc. rate=0.765] Warmup: 2%|1 | 17/1050 [00:00, 102.76it/s, step size=6.67e-01, acc. rate=0.778] Warmup: 2%|1 | 18/1050 [00:00, 102.76it/s, step size=1.24e+00, acc. rate=0.789] Warmup: 2%|1 | 19/1050 [00:00, 102.76it/s, step size=1.15e-01, acc. rate=0.750] Warmup: 2%|1 | 20/1050 [00:00, 102.76it/s, step size=2.13e-01, acc. rate=0.762] Warmup: 2%|2 | 21/1050 [00:00, 99.97it/s, step size=2.13e-01, acc. rate=0.762] Warmup: 2%|2 | 21/1050 [00:00, 99.97it/s, step size=3.92e-01, acc. rate=0.773] Warmup: 2%|2 | 22/1050 [00:00, 99.97it/s, step size=6.74e-01, acc. rate=0.783] Warmup: 2%|2 | 23/1050 [00:00, 99.97it/s, step size=8.90e-01, acc. rate=0.792] Warmup: 2%|2 | 24/1050 [00:00, 99.97it/s, step size=1.60e+00, acc. rate=0.800] Warmup: 2%|2 | 25/1050 [00:00, 99.97it/s, step size=1.67e-01, acc. rate=0.769] Warmup: 2%|2 | 26/1050 [00:00, 99.97it/s, step size=3.02e-01, acc. rate=0.778] Warmup: 3%|2 | 27/1050 [00:00, 99.97it/s, step size=5.38e-01, acc. rate=0.786] Warmup: 3%|2 | 28/1050 [00:00, 99.97it/s, step size=9.52e-01, acc. rate=0.793] Warmup: 3%|2 | 29/1050 [00:00, 99.97it/s, step size=1.67e+00, acc. rate=0.800] Warmup: 3%|2 | 30/1050 [00:00, 99.97it/s, step size=1.91e-01, acc. rate=0.774] Warmup: 3%|2 | 31/1050 [00:00, 99.97it/s, step size=3.34e-01, acc. rate=0.781] Warmup: 3%|3 | 32/1050 [00:00, 99.97it/s, step size=4.76e-01, acc. rate=0.788] Warmup: 3%|3 | 33/1050 [00:00, 99.97it/s, step size=7.42e-01, acc. rate=0.794] Warmup: 3%|3 | 34/1050 [00:00, 107.14it/s, step size=7.42e-01, acc. rate=0.794] Warmup: 3%|3 | 34/1050 [00:00, 107.14it/s, step size=1.27e+00, acc. rate=0.800] Warmup: 3%|3 | 35/1050 [00:00, 107.14it/s, step size=1.59e-01, acc. rate=0.778] Warmup: 3%|3 | 36/1050 [00:00, 107.14it/s, step size=2.64e-01, acc. rate=0.784] Warmup: 4%|3 | 37/1050 [00:00, 107.14it/s, step size=4.51e-01, acc. rate=0.789] Warmup: 4%|3 | 38/1050 [00:00, 107.14it/s, step size=7.64e-01, acc. rate=0.795] Warmup: 4%|3 | 39/1050 [00:00, 107.14it/s, step size=9.19e-01, acc. rate=0.800] Warmup: 4%|3 | 40/1050 [00:00, 107.14it/s, step size=1.69e-01, acc. rate=0.780] Warmup: 4%|3 | 41/1050 [00:00, 107.14it/s, step size=2.85e-01, acc. rate=0.786] Warmup: 4%|4 | 42/1050 [00:00, 107.14it/s, step size=4.49e-01, acc. rate=0.791] Warmup: 4%|4 | 43/1050 [00:00, 107.14it/s, step size=6.73e-01, acc. rate=0.795] Warmup: 4%|4 | 44/1050 [00:00, 107.14it/s, step size=1.59e+00, acc. rate=0.778] Warmup: 4%|4 | 45/1050 [00:00, 107.14it/s, step size=2.29e+01, acc. rate=0.783] Warmup: 4%|4 | 46/1050 [00:00, 107.14it/s, step size=3.87e+00, acc. rate=0.766] Warmup: 4%|4 | 47/1050 [00:00, 107.14it/s, step size=3.82e-01, acc. rate=0.750] Warmup: 5%|4 | 48/1050 [00:00, 107.14it/s, step size=5.17e-01, acc. rate=0.755] Warmup: 5%|4 | 49/1050 [00:00, 116.05it/s, step size=5.17e-01, acc. rate=0.755] Warmup: 5%|4 | 49/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.760] Sample: 5%|4 | 50/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.760] Sample: 5%|4 | 50/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.765] Sample: 5%|4 | 51/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.769] Sample: 5%|4 | 52/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.774] Sample: 5%|5 | 53/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.778] Sample: 5%|5 | 54/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.782] Sample: 5%|5 | 55/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.786] Sample: 5%|5 | 56/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.789] Sample: 5%|5 | 57/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.793] Sample: 6%|5 | 58/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.797] Sample: 6%|5 | 59/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.800] Sample: 6%|5 | 60/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.803] Sample: 6%|5 | 61/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.806] Sample: 6%|5 | 62/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.810] Sample: 6%|6 | 63/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.812] Sample: 6%|6 | 64/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.815] Sample: 6%|6 | 65/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.818] Sample: 6%|6 | 66/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.821] Sample: 6%|6 | 67/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.824] Sample: 6%|6 | 68/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.826] Sample: 7%|6 | 69/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.829] Sample: 7%|6 | 70/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.831] Sample: 7%|6 | 71/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.833] Sample: 7%|6 | 72/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.836] Sample: 7%|6 | 73/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.838] Sample: 7%|7 | 74/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.840] Sample: 7%|7 | 75/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.842] Sample: 7%|7 | 76/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.844] Sample: 7%|7 | 77/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.846] Sample: 7%|7 | 78/1050 [00:00, 116.05it/s, step size=1.11e+00, acc. rate=0.848] Sample: 8%|7 | 79/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.848] Sample: 8%|7 | 79/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.850] Sample: 8%|7 | 80/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.852] Sample: 8%|7 | 81/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.854] Sample: 8%|7 | 82/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.855] Sample: 8%|7 | 83/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.857] Sample: 8%|8 | 84/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.859] Sample: 8%|8 | 85/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.860] Sample: 8%|8 | 86/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.862] Sample: 8%|8 | 87/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.864] Sample: 8%|8 | 88/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.865] Sample: 8%|8 | 89/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.867] Sample: 9%|8 | 90/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.868] Sample: 9%|8 | 91/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.870] Sample: 9%|8 | 92/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.860] Sample: 9%|8 | 93/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.862] Sample: 9%|8 | 94/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.863] Sample: 9%|9 | 95/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.865] Sample: 9%|9 | 96/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.866] Sample: 9%|9 | 97/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.867] Sample: 9%|9 | 98/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.869] Sample: 9%|9 | 99/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.870] Sample: 10%|9 | 100/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.871] Sample: 10%|9 | 101/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.873] Sample: 10%|9 | 102/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.874] Sample: 10%|9 | 103/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.875] Sample: 10%|9 | 104/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.876] Sample: 10%|# | 105/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.877] Sample: 10%|# | 106/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.879] Sample: 10%|# | 107/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.880] Sample: 10%|# | 108/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.872] Sample: 10%|# | 109/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.873] Sample: 10%|# | 110/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.874] Sample: 11%|# | 111/1050 [00:00, 142.20it/s, step size=1.11e+00, acc. rate=0.875] Sample: 11%|# | 112/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.875] Sample: 11%|# | 112/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.876] Sample: 11%|# | 113/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.877] Sample: 11%|# | 114/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.878] Sample: 11%|# | 115/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.879] Sample: 11%|#1 | 116/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.880] Sample: 11%|#1 | 117/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.881] Sample: 11%|#1 | 118/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.882] Sample: 11%|#1 | 119/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.883] Sample: 11%|#1 | 120/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.884] Sample: 12%|#1 | 121/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.885] Sample: 12%|#1 | 122/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.886] Sample: 12%|#1 | 123/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.887] Sample: 12%|#1 | 124/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.888] Sample: 12%|#1 | 125/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.889] Sample: 12%|#2 | 126/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.890] Sample: 12%|#2 | 127/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.883] Sample: 12%|#2 | 128/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.884] Sample: 12%|#2 | 129/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.885] Sample: 12%|#2 | 130/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.885] Sample: 12%|#2 | 131/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.886] Sample: 13%|#2 | 132/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.887] Sample: 13%|#2 | 133/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.888] Sample: 13%|#2 | 134/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.889] Sample: 13%|#2 | 135/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.890] Sample: 13%|#2 | 136/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.891] Sample: 13%|#3 | 137/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.891] Sample: 13%|#3 | 138/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.892] Sample: 13%|#3 | 139/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.893] Sample: 13%|#3 | 140/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.894] Sample: 13%|#3 | 141/1050 [00:00, 170.77it/s, step size=1.11e+00, acc. rate=0.894] Sample: 14%|#3 | 142/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.894] Sample: 14%|#3 | 142/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.895] Sample: 14%|#3 | 143/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.896] Sample: 14%|#3 | 144/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.897] Sample: 14%|#3 | 145/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.897] Sample: 14%|#3 | 146/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.898] Sample: 14%|#4 | 147/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.899] Sample: 14%|#4 | 148/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.899] Sample: 14%|#4 | 149/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.900] Sample: 14%|#4 | 150/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.901] Sample: 14%|#4 | 151/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.901] Sample: 14%|#4 | 152/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.902] Sample: 15%|#4 | 153/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.903] Sample: 15%|#4 | 154/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.903] Sample: 15%|#4 | 155/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.904] Sample: 15%|#4 | 156/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.904] Sample: 15%|#4 | 157/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.905] Sample: 15%|#5 | 158/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.906] Sample: 15%|#5 | 159/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.906] Sample: 15%|#5 | 160/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.907] Sample: 15%|#5 | 161/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.907] Sample: 15%|#5 | 162/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.908] Sample: 16%|#5 | 163/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.909] Sample: 16%|#5 | 164/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.909] Sample: 16%|#5 | 165/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.910] Sample: 16%|#5 | 166/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.910] Sample: 16%|#5 | 167/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.911] Sample: 16%|#6 | 168/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.911] Sample: 16%|#6 | 169/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.912] Sample: 16%|#6 | 170/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.912] Sample: 16%|#6 | 171/1050 [00:00, 195.83it/s, step size=1.11e+00, acc. rate=0.913] Sample: 16%|#6 | 172/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.913] Sample: 16%|#6 | 172/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.913] Sample: 16%|#6 | 173/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.914] Sample: 17%|#6 | 174/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.914] Sample: 17%|#6 | 175/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.915] Sample: 17%|#6 | 176/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.915] Sample: 17%|#6 | 177/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.916] Sample: 17%|#6 | 178/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.916] Sample: 17%|#7 | 179/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.917] Sample: 17%|#7 | 180/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.917] Sample: 17%|#7 | 181/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.918] Sample: 17%|#7 | 182/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.918] Sample: 17%|#7 | 183/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.918] Sample: 18%|#7 | 184/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.919] Sample: 18%|#7 | 185/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.919] Sample: 18%|#7 | 186/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.920] Sample: 18%|#7 | 187/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.920] Sample: 18%|#7 | 188/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.921] Sample: 18%|#8 | 189/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.921] Sample: 18%|#8 | 190/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.921] Sample: 18%|#8 | 191/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.922] Sample: 18%|#8 | 192/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.922] Sample: 18%|#8 | 193/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.923] Sample: 18%|#8 | 194/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.923] Sample: 19%|#8 | 195/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.923] Sample: 19%|#8 | 196/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.924] Sample: 19%|#8 | 197/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.924] Sample: 19%|#8 | 198/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.925] Sample: 19%|#8 | 199/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.925] Sample: 19%|#9 | 200/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.925] Sample: 19%|#9 | 201/1050 [00:00, 217.20it/s, step size=1.11e+00, acc. rate=0.926] Sample: 19%|#9 | 202/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.926] Sample: 19%|#9 | 202/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.926] Sample: 19%|#9 | 203/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.926] Sample: 19%|#9 | 204/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.927] Sample: 20%|#9 | 205/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.927] Sample: 20%|#9 | 206/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.928] Sample: 20%|#9 | 207/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.928] Sample: 20%|#9 | 208/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.928] Sample: 20%|#9 | 209/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.929] Sample: 20%|## | 210/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.929] Sample: 20%|## | 211/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.929] Sample: 20%|## | 212/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.930] Sample: 20%|## | 213/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.930] Sample: 20%|## | 214/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.930] Sample: 20%|## | 215/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.931] Sample: 21%|## | 216/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.931] Sample: 21%|## | 217/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.931] Sample: 21%|## | 218/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|## | 219/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|## | 220/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|##1 | 221/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|##1 | 222/1050 [00:00, 235.62it/s, step size=1.11e+00, acc. rate=0.933] Sample: 21%|##1 | 223/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.933] Sample: 21%|##1 | 224/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.933] Sample: 21%|##1 | 225/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.934] Sample: 22%|##1 | 226/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.934] Sample: 22%|##1 | 227/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.934] Sample: 22%|##1 | 228/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.934] Sample: 22%|##1 | 229/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.935] Sample: 22%|##1 | 230/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.935] Sample: 22%|##2 | 231/1050 [00:01, 235.62it/s, step size=1.11e+00, acc. rate=0.935] Sample: 22%|##2 | 232/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.935] Sample: 22%|##2 | 232/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.936] Sample: 22%|##2 | 233/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.936] Sample: 22%|##2 | 234/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.932] Sample: 22%|##2 | 235/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.932] Sample: 22%|##2 | 236/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.932] Sample: 23%|##2 | 237/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.933] Sample: 23%|##2 | 238/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.933] Sample: 23%|##2 | 239/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.929] Sample: 23%|##2 | 240/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.929] Sample: 23%|##2 | 241/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.930] Sample: 23%|##3 | 242/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.930] Sample: 23%|##3 | 243/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.930] Sample: 23%|##3 | 244/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.931] Sample: 23%|##3 | 245/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.931] Sample: 23%|##3 | 246/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.931] Sample: 24%|##3 | 247/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.931] Sample: 24%|##3 | 248/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.932] Sample: 24%|##3 | 249/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.932] Sample: 24%|##3 | 250/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.932] Sample: 24%|##3 | 251/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.933] Sample: 24%|##4 | 252/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.933] Sample: 24%|##4 | 253/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.933] Sample: 24%|##4 | 254/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.933] Sample: 24%|##4 | 255/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.934] Sample: 24%|##4 | 256/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.934] Sample: 24%|##4 | 257/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.934] Sample: 25%|##4 | 258/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.934] Sample: 25%|##4 | 259/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##4 | 260/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##4 | 261/1050 [00:01, 250.30it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##4 | 262/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##4 | 262/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##5 | 263/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.936] Sample: 25%|##5 | 264/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.936] Sample: 25%|##5 | 265/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.936] Sample: 25%|##5 | 266/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.936] Sample: 25%|##5 | 267/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.937] Sample: 26%|##5 | 268/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.933] Sample: 26%|##5 | 269/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.933] Sample: 26%|##5 | 270/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.934] Sample: 26%|##5 | 271/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.934] Sample: 26%|##5 | 272/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.934] Sample: 26%|##6 | 273/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.934] Sample: 26%|##6 | 274/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.935] Sample: 26%|##6 | 275/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.935] Sample: 26%|##6 | 276/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.935] Sample: 26%|##6 | 277/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.935] Sample: 26%|##6 | 278/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.935] Sample: 27%|##6 | 279/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.936] Sample: 27%|##6 | 280/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.936] Sample: 27%|##6 | 281/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.936] Sample: 27%|##6 | 282/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.936] Sample: 27%|##6 | 283/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.937] Sample: 27%|##7 | 284/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.937] Sample: 27%|##7 | 285/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.937] Sample: 27%|##7 | 286/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.937] Sample: 27%|##7 | 287/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.938] Sample: 27%|##7 | 288/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.938] Sample: 28%|##7 | 289/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.938] Sample: 28%|##7 | 290/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.938] Sample: 28%|##7 | 291/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.938] Sample: 28%|##7 | 292/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##7 | 293/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 294/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 295/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 296/1050 [00:01, 263.19it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 297/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 297/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.940] Sample: 28%|##8 | 298/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.940] Sample: 28%|##8 | 299/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.940] Sample: 29%|##8 | 300/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.940] Sample: 29%|##8 | 301/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.940] Sample: 29%|##8 | 302/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##8 | 303/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##8 | 304/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##9 | 305/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##9 | 306/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##9 | 307/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 29%|##9 | 308/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 29%|##9 | 309/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 30%|##9 | 310/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 30%|##9 | 311/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 30%|##9 | 312/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 30%|##9 | 313/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|##9 | 314/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|### | 315/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|### | 316/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|### | 317/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|### | 318/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.944] Sample: 30%|### | 319/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.944] Sample: 30%|### | 320/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.944] Sample: 31%|### | 321/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.941] Sample: 31%|### | 322/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.941] Sample: 31%|### | 323/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.941] Sample: 31%|### | 324/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|### | 325/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 326/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 327/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 328/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 329/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 330/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 331/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 332/1050 [00:01, 282.85it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 333/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 333/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 334/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 335/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###2 | 336/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 337/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 338/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 339/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 340/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 341/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 33%|###2 | 342/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###2 | 343/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###2 | 344/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###2 | 345/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###2 | 346/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###3 | 347/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###3 | 348/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.946] Sample: 33%|###3 | 349/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.943] Sample: 33%|###3 | 350/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.943] Sample: 33%|###3 | 351/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.943] Sample: 34%|###3 | 352/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.943] Sample: 34%|###3 | 353/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###3 | 354/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###3 | 355/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###3 | 356/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###4 | 357/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###4 | 358/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###4 | 359/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###4 | 360/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 34%|###4 | 361/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 34%|###4 | 362/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 363/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 364/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 365/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 366/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###4 | 367/1050 [00:01, 300.32it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 368/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 368/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 369/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 370/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 371/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 372/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.946] Sample: 36%|###5 | 373/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###5 | 374/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###5 | 375/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###5 | 376/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###5 | 377/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###6 | 378/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###6 | 379/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###6 | 380/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.948] Sample: 36%|###6 | 381/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.948] Sample: 36%|###6 | 382/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.948] Sample: 36%|###6 | 383/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 384/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 385/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 386/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 387/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 388/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 389/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 390/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 391/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 392/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 393/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.949] Sample: 38%|###7 | 394/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.949] Sample: 38%|###7 | 395/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.949] Sample: 38%|###7 | 396/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###7 | 397/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###7 | 398/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 399/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 400/1050 [00:01, 310.92it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 401/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 401/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 402/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 403/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 404/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 405/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 406/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 407/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 408/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 409/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###9 | 410/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###9 | 411/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###9 | 412/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 39%|###9 | 413/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 39%|###9 | 414/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 415/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 416/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 417/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 418/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 419/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|#### | 420/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|#### | 421/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 40%|#### | 422/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 40%|#### | 423/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 40%|#### | 424/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 40%|#### | 425/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 426/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 427/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 428/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 429/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 430/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 431/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 432/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 433/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 434/1050 [00:01, 315.80it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 435/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 435/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 436/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 437/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 438/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 439/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####1 | 440/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 441/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 442/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 443/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 444/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 445/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 446/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 43%|####2 | 447/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 43%|####2 | 448/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 43%|####2 | 449/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####2 | 450/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####2 | 451/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 452/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 453/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 454/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 455/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 456/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 44%|####3 | 457/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 44%|####3 | 458/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 44%|####3 | 459/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####3 | 460/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####3 | 461/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 462/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 463/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 464/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 465/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 466/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 467/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 45%|####4 | 468/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 45%|####4 | 469/1050 [00:01, 320.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 45%|####4 | 470/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 45%|####4 | 470/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 45%|####4 | 471/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 45%|####4 | 472/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 473/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 474/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 475/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 476/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 477/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 478/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 479/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 480/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 481/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 482/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 483/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 484/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 485/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 486/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 487/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 488/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 489/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 490/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 491/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 492/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 493/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####7 | 494/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 47%|####7 | 495/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 47%|####7 | 496/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 47%|####7 | 497/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 47%|####7 | 498/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 499/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 500/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 501/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 502/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 503/1050 [00:01, 326.50it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####8 | 504/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####8 | 504/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####8 | 505/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####8 | 506/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.959] Sample: 48%|####8 | 507/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.959] Sample: 48%|####8 | 508/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.959] Sample: 48%|####8 | 509/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 510/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 511/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 512/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 513/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.957] Sample: 49%|####8 | 514/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 49%|####9 | 515/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 49%|####9 | 516/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 49%|####9 | 517/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 49%|####9 | 518/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 49%|####9 | 519/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 520/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 521/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 522/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 523/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 524/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|##### | 525/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|##### | 526/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|##### | 527/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|##### | 528/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.957] Sample: 50%|##### | 529/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 50%|##### | 530/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 531/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 532/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 533/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 534/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 535/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 536/1050 [00:01, 320.62it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 537/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 537/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 538/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 539/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 51%|#####1 | 540/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 52%|#####1 | 541/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 52%|#####1 | 542/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 52%|#####1 | 543/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####1 | 544/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####1 | 545/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 546/1050 [00:01, 313.75it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 547/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 548/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 549/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 52%|#####2 | 550/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 52%|#####2 | 551/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 552/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 553/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 554/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 555/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 556/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 557/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 558/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 559/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 560/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 561/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 562/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 563/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 564/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 565/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 566/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 567/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 568/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 569/1050 [00:02, 313.75it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 570/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 570/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 571/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 572/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.956] Sample: 55%|#####4 | 573/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.956] Sample: 55%|#####4 | 574/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####4 | 575/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####4 | 576/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####4 | 577/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 578/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 579/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 580/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 581/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 582/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 583/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 584/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 585/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 586/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 587/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####6 | 588/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 589/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 590/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 591/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 592/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 593/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 594/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 595/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 596/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 597/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 598/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####7 | 599/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####7 | 600/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####7 | 601/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####7 | 602/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.959] Sample: 57%|#####7 | 603/1050 [00:02, 316.51it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 604/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 604/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 605/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 606/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 607/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 608/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 609/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 610/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 611/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 612/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 613/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 614/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 59%|#####8 | 615/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 59%|#####8 | 616/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.959] Sample: 59%|#####8 | 617/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####8 | 618/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####8 | 619/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 620/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 621/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 622/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 623/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 624/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 625/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 626/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 627/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 628/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 629/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|###### | 630/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|###### | 631/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|###### | 632/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.961] Sample: 60%|###### | 633/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.961] Sample: 60%|###### | 634/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.961] Sample: 60%|###### | 635/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 636/1050 [00:02, 321.79it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 637/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 637/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 638/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 639/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 640/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 641/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 642/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 643/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 644/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 645/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 62%|######1 | 646/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 62%|######1 | 647/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 62%|######1 | 648/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.961] Sample: 62%|######1 | 649/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######1 | 650/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 651/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 652/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 653/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 654/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 655/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 656/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 657/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 658/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 659/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 660/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 661/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 662/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 663/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 664/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 665/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 666/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 667/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 668/1050 [00:02, 281.32it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 669/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 669/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 670/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 671/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 672/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 673/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 674/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 675/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 676/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 677/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 678/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 679/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 680/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 681/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 682/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######5 | 683/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######5 | 684/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 65%|######5 | 685/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 65%|######5 | 686/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 65%|######5 | 687/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 688/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 689/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 690/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 691/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 692/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 693/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 694/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 695/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 696/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 697/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 698/1050 [00:02, 290.70it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 699/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 699/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 700/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 701/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 702/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 703/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######7 | 704/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 67%|######7 | 705/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 67%|######7 | 706/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 67%|######7 | 707/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 67%|######7 | 708/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 709/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 710/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 711/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 712/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 713/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 714/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 715/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 716/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 717/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 718/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 719/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 720/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 721/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 722/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 723/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 724/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 725/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 726/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 727/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 728/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 729/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 730/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 731/1050 [00:02, 289.54it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 732/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 732/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 733/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 734/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 735/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 736/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 737/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 738/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 739/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 740/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 741/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 742/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 743/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 744/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 745/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|#######1 | 746/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 71%|#######1 | 747/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 71%|#######1 | 748/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 71%|#######1 | 749/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 71%|#######1 | 750/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 751/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 752/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 753/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 754/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 755/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 756/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 757/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 758/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 759/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 760/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 761/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 762/1050 [00:02, 299.64it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 763/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 763/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 764/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 765/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 766/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######3 | 767/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######3 | 768/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######3 | 769/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 73%|#######3 | 770/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 73%|#######3 | 771/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 772/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 773/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 774/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 775/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 776/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 777/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 778/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 779/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 780/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 781/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 782/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 783/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 784/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 785/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 786/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 787/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 788/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 789/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 790/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 791/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 792/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.968] Sample: 76%|#######5 | 793/1050 [00:02, 296.60it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 794/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 794/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 795/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 796/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 797/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 798/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 799/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 800/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 801/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 802/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 803/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 804/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 805/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 806/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 807/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 808/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 809/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 810/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 811/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 812/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 813/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 814/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 815/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 816/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 817/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 818/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######8 | 819/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 820/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 821/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 822/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 823/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 824/1050 [00:02, 298.22it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 825/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 825/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 826/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 827/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 828/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 829/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 830/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 831/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 832/1050 [00:02, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 833/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 834/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 835/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 836/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 837/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 838/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 839/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 840/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 841/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 842/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 843/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 844/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 845/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 81%|######## | 846/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.970] Sample: 81%|######## | 847/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|######## | 848/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|######## | 849/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|######## | 850/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 851/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 852/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 853/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 854/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 855/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 856/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 857/1050 [00:03, 273.76it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 858/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 858/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 859/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 860/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 861/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 862/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 863/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 864/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 865/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 866/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 867/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 868/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 869/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 870/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 871/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 872/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 873/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 874/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 875/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 876/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.971] Sample: 84%|########3 | 877/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########3 | 878/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########3 | 879/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########3 | 880/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########3 | 881/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 882/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 883/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 884/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 885/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 886/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 887/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 888/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 889/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 890/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 891/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 892/1050 [00:03, 288.37it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 893/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 893/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 894/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 895/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 896/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 897/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 898/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 899/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 900/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 901/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 902/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########6 | 903/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########6 | 904/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.971] Sample: 86%|########6 | 905/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.971] Sample: 86%|########6 | 906/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.971] Sample: 86%|########6 | 907/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.971] Sample: 86%|########6 | 908/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.971] Sample: 87%|########6 | 909/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.971] Sample: 87%|########6 | 910/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.971] Sample: 87%|########6 | 911/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.971] Sample: 87%|########6 | 912/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########6 | 913/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 914/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 915/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 916/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 917/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 918/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 919/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 920/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 921/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 922/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 923/1050 [00:03, 302.51it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 924/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 924/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 925/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 926/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 927/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 928/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 929/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 930/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 931/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 932/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 933/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 934/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 935/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 936/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 937/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 938/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 939/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 940/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 941/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 942/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 943/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 944/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|######### | 945/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 946/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 947/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 948/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 949/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 950/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 951/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 952/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 953/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 954/1050 [00:03, 303.56it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 955/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 955/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 956/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 957/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 958/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 959/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 960/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 961/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 962/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 963/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 964/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 965/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 966/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 967/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 968/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 969/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 970/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 971/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 972/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 973/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 974/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 975/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 976/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 977/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 978/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 979/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 980/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 981/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 982/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 983/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 984/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 985/1050 [00:03, 304.64it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 986/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 986/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 987/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 988/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 989/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 990/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 991/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 992/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 993/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 994/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 995/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 996/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 997/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 998/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 999/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 1000/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 1001/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 1002/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1003/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1004/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1005/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1006/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1007/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1008/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1009/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1010/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1011/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1012/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1013/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1014/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1015/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1016/1050 [00:03, 305.13it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1017/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1017/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1018/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1019/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1020/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1021/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1022/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1023/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1024/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1025/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1026/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1027/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1028/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1029/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1030/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1031/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1032/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1033/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1034/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1035/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1036/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1037/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1038/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1039/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1040/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1041/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1042/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1043/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1044/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1045/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1046/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1047/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1048/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1049/1050 [00:03, 291.27it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|##########| 1050/1050 [00:03, 284.53it/s, step size=1.11e+00, acc. rate=0.974] marginal = EmpiricalMarginal(posterior, &quot;weight&quot;) plt.hist([marginal().item() for _ in range(1000)],) ## (array([ 11., 62., 113., 167., 104., 143., 134., 141., 91., 34.]), array([10.89175224, 11.21284065, 11.53392906, 11.85501747, 12.17610588, ## 12.49719429, 12.8182827 , 13.13937111, 13.46045952, 13.78154793, ## 14.10263634]), &lt;a list of 10 Patch objects&gt;) plt.title(&quot;P(weight | measurement = 14)&quot;) ## Text(0.5, 1.0, &#39;P(weight | measurement = 14)&#39;) plt.xlabel(&quot;Weight&quot;) ## Text(0.5, 0, &#39;Weight&#39;) plt.ylabel(&quot;#&quot;) ## Text(0, 0.5, &#39;#&#39;) 4.3.0.1 Shapes in distribution: We know that PyTorch tensor have single shape attribute, Distributions have two shape attributes with special meaning. * .batch_shape: Indices over .batch_shape denote conditionally independent random variables, * .event_shape: indices over .event_shape denote dependent random variables (ie one draw from a distribution). These two combine to define the total shape of a sample. Thus the total shape of .log_prob() of distribution is .batch_shape. Also, Distribution.sample() also has a sample_shape attribute that indexes over independent and identically distributed(iid) random variables. | iid | independent | dependent ------+--------------+-------------+------------ shape = sample_shape + batch_shape + event_shape To know more about + , go through broadcasting tensors in PyTorch. 4.3.1 Examples One way to introduce batch_shape is use expand. d = dist.MultivariateNormal(torch.zeros(3), torch.eye(3, 3)).expand([5]) # expand - 3 of these Multivariate Normal Dists print(&quot;batch_shape: &quot;, d.batch_shape) ## batch_shape: torch.Size([5]) print(&quot;event_shape: &quot;, d.event_shape) #x = d.sample(torch.Size([5])) ## event_shape: torch.Size([3]) x = d.sample() print(&quot;x shape: &quot;, x.shape) # == sample_shape + batch_shape + event_shape ## x shape: torch.Size([5, 3]) print(&quot;d.log_prob(x) shape:&quot;, d.log_prob(x).shape) # == batch_shape ## d.log_prob(x) shape: torch.Size([5]) The other way is using plate context manager. Pyro models can use the context manager pyro.plate to declare that certain batch dimensions are independent. Inference algorithms can then take advantage of this independence to e.g. construct lower variance gradient estimators or to enumerate in linear space rather than exponential space. with pyro.plate(&quot;x_axis&quot;, 5): d = dist.MultivariateNormal(torch.zeros(3), torch.eye(3, 3)) x = pyro.sample(&quot;x&quot;, d) x.shape ## torch.Size([5, 3]) In fact, we can also nest plates. The only thing we need to care about is, which dimensions are independent. Pyro automatically manages this but sometimes we need to explicitely specify the dimensions. Once we specify that, we can leverage PyTorch’s CUDA enabled capabilities to run inference on GPUs. with pyro.plate(&quot;x_axis&quot;, 320): # within this context, batch dimension -1 is independent with pyro.plate(&quot;y_axis&quot;, 200): # within this context, batch dimensions -2 and -1 are independent Note that we always count from the right by using negative indices like \\(-2\\), \\(-1\\). 4.3.2 Gaussian Mixture Model \\[\\texttt{Blei - Build, Compute, Critique, Repeat:Data Analysis with Latent Variable Models}\\] from __future__ import print_function import os from collections import defaultdict import numpy as np import scipy.stats import torch from torch.distributions import constraints from pyro import poutine from pyro.contrib.autoguide import AutoDelta from pyro.optim import Adam from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate, infer_discrete from matplotlib import pyplot # %matplotlib inline pyro.enable_validation(True) data = torch.tensor([0., 1., 10., 11., 12.]) K = 2 # Fixed number of components. @config_enumerate def model(data): # Global variables. weights = pyro.sample(&#39;weights&#39;, dist.Dirichlet(0.5 * torch.ones(K))) scale = pyro.sample(&#39;scale&#39;, dist.LogNormal(0., 2.)) with pyro.plate(&#39;components&#39;, K): locs = pyro.sample(&#39;locs&#39;, dist.Normal(0., 10.)) with pyro.plate(&#39;data&#39;, len(data)): # Local variables. assignment = pyro.sample(&#39;assignment&#39;, dist.Categorical(weights)) pyro.sample(&#39;obs&#39;, dist.Normal(locs[assignment], scale), obs=data) 4.3.3 Review of Approximate Inference We have variables \\(Z\\)s (cluster assignments) and \\(X\\)s (data points) in our mixture model, where \\(X\\) is observed and \\(Z\\) is latent (unobserved). As we saw earlier, a generative model entails a joint distribution \\[p(Z,X)\\] Inference of unknown can be achieved through conditioning on the observations. \\[p(Z \\mid X) = \\frac{p(Z, X)}{p(X)}\\] And for the most interesting problems, the integral for the denominator(marginal) is not tractable. \\[p(X) = \\int dZp(X \\mid Z)p(Z)\\] So we have to directly approximate \\(p(Z \\mid X)\\). There are two ways of approximate this posterior. Sampling methods like Gibbs sampler. Variational inference. 4.3.4 Variational Inference: We can’t compute \\(p(Z \\mid X)\\) directly, so let’s approximate with some other distribution \\(q(Z; \\nu)\\) over Z that is tractable (for example, Gaussions or other exponential family). Image \\[\\texttt{David Blei - Variational Inference (NeurIPS 2016 Tutorial)}\\] Since q is tractable, we can play with it’s parameter \\(\\nu\\) such that it reaches as close to \\(p(Z\\mid X)\\) as possible. More precisely, we want to minimize the KL divergence between \\(q\\) and \\(p\\). With this trick, we just turned an inference problem to an optimization problem! \\[ \\begin{align*} KL(q(Z;\\nu) \\mid\\mid p(Z\\mid X)) &amp;= -\\int dZ\\ q(Z) \\log\\frac{P(Z\\mid X)}{q(Z)}\\\\ &amp;= -\\int dZ\\ q(Z) \\log \\frac{\\frac{p(Z,X)}{p(X)}}{q(Z)}\\\\ &amp;= -\\int dZ\\ q(Z) \\log \\frac{p(Z,X)}{p(X)q(Z)}\\\\ &amp;= -\\int dZ\\ q(Z) \\left[ \\log \\frac{p(Z,X)}{q(Z)} - \\log p(X) \\right]\\\\ &amp;= - \\int dZ\\ \\log \\frac{p(Z,X)}{q(Z)} + \\underbrace{\\int dZ\\ q(Z)}_{\\text{=1}}\\log p(X)\\\\ &amp;= - \\int dZ\\ \\log \\frac{p(Z,X)}{q(Z)} + \\log p(X)\\\\ \\log p(X) &amp;= KL(q(Z;\\nu)\\mid\\mid p(Z\\mid X) + \\underbrace{\\int dZ\\ q(Z;\\nu) \\log \\frac{p(Z,X)}{q(Z;\\nu)}}_{\\mathcal{L}}\\\\ \\end{align*} \\] Note that we already observed \\(X\\) and we conditioned the model to get \\(p(Z \\mid X)\\). But given \\(X\\), \\(\\log p(X)\\) is constant! So, minimizing KL is equivalent to maximizing \\(\\mathcal{L}\\). How do you maximize \\(\\mathcal{L}\\)? Take \\(\\nabla_{\\nu} \\mathcal{L}\\). \\(\\mathcal{L}\\) is called variational lower bound. It is often called ELBO. Stochastic Variational Inference scales variational inference to massive data. Just like in stochastic variational inference, you subsample the data and update the posterior! 4.3.5 Stochastic Optimization In stochastic optimization, we replace the gradient with cheaper noisy estimate which is guranteed to converge to a local optimum. \\[\\nu_{t+1} = \\nu_t + \\rho_t \\hat{\\nabla}_{\\nu} \\mathcal{L}(\\nu_t)\\] Requirements: Unbiased gradients, i.e. \\[\\mathbb{E}[\\hat{\\nabla}_{\\nu} \\mathcal{L}(\\nu_t)] = \\nabla_{\\nu}\\mathcal{L}(\\nu)\\] Step-size sequence \\(\\rho_t\\) that follows Robbins-Monro conditions. Stochastic variational inference takes inspiration from stochastic optimization and natural graidient. We follow the same procedure as stochastic gradient descent. 4.3.6 A Rough Stochastic variational inference algorithm: Initialize \\(q\\) with some \\(\\nu\\) Until Converge: subsample from Data: compute gradient \\(\\hat{\\nabla_{\\nu}}\\mathcal{L}_{\\nu_t}\\) update global parameter \\(\\nu_{t+1} = \\nu_t + \\rho_t \\hat{\\nabla_{\\nu}}\\mathcal{L}_{\\nu_t}\\) Return \\(q(Z;\\nu)\\) 4.3.7 Training a MAP estimator Let’s start by learning model parameters weights, locs, and scale given priors and data. We will use AutoDelta guide function. Our model will learn global mixture weights, the location of each mixture component, and a shared scale that is common to both components. During inference, TraceEnum_ELBO will marginalize out the assignments of datapoints to clusters. max_plate_nesting lets Pyro know that we’re using the rightmost dimension plate and that Pyro can use any other dimension for parallelization. 4.4 Some other Pyro vocabulary poutine - Beneath the built-in inference algorithms, Pyro has a library of composable effect handlers for creating new inference algorithms and working with probabilistic programs. Pyro’s inference algorithms are all built by applying these handlers to stochastic functions. poutine.block - blocks pyro premitives. By default, it blocks everything. param - Parameters in Pyro are basically thin wrappers around PyTorch Tensors that carry unique names. As such Parameters are the primary stateful objects in Pyro. Users typically interact with parameters via the Pyro primitive pyro.param. Parameters play a central role in stochastic variational inference, where they are used to represent point estimates for the parameters in parameterized families of models and guides. param_store - Global store for parameters in Pyro. This is basically a key-value store. global_guide = AutoDelta(poutine.block(model, expose=[&#39;weights&#39;, &#39;locs&#39;, &#39;scale&#39;])) optim = pyro.optim.Adam({&#39;lr&#39;: 0.1, &#39;betas&#39;: [0.8, 0.99]}) elbo = TraceEnum_ELBO(max_plate_nesting=1) svi = SVI(model, global_guide, optim, loss=elbo) def initialize(seed): pyro.set_rng_seed(seed) pyro.clear_param_store() # Initialize weights to uniform. pyro.param(&#39;auto_weights&#39;, 0.5 * torch.ones(K), constraint=constraints.simplex) # Assume half of the data variance is due to intra-component noise. pyro.param(&#39;auto_scale&#39;, (data.var() / 2).sqrt(), constraint=constraints.positive) # Initialize means from a subsample of data. pyro.param(&#39;auto_locs&#39;, data[torch.multinomial(torch.ones(len(data)) / len(data), K)]); loss = svi.loss(model, global_guide, data) return loss # Choose the best among 100 random initializations. loss, seed = min((initialize(seed), seed) for seed in range(100)) initialize(seed) ## 25.665584564208984 print(&#39;seed = {}, initial_loss = {}&#39;.format(seed, loss)) ## seed = 7, initial_loss = 25.665584564208984 # Register hooks to monitor gradient norms. gradient_norms = defaultdict(list) for name, value in pyro.get_param_store().named_parameters(): value.register_hook(lambda g, name=name: gradient_norms[name].append(g.norm().item())) ## &lt;torch.utils.hooks.RemovableHandle object at 0x119f62470&gt; ## &lt;torch.utils.hooks.RemovableHandle object at 0x1308b8f60&gt; ## &lt;torch.utils.hooks.RemovableHandle object at 0x119f62470&gt; losses = [] for i in range(200): loss = svi.step(data) losses.append(loss) print(&#39;.&#39; if i % 100 else &#39;\\n&#39;, end=&#39;&#39;) ## ## ................................................................................................... ## ................................................................................................... pyplot.figure(figsize=(10,3), dpi=100).set_facecolor(&#39;white&#39;) pyplot.plot(losses) ## [&lt;matplotlib.lines.Line2D object at 0x1a325b9f60&gt;] pyplot.xlabel(&#39;iters&#39;) ## Text(0.5, 0, &#39;iters&#39;) pyplot.ylabel(&#39;loss&#39;) ## Text(0, 0.5, &#39;loss&#39;) pyplot.yscale(&#39;log&#39;) pyplot.title(&#39;Convergence of SVI&#39;); ## Text(0.5, 1.0, &#39;Convergence of SVI&#39;) map_estimates = global_guide(data) weights = map_estimates[&#39;weights&#39;] locs = map_estimates[&#39;locs&#39;] scale = map_estimates[&#39;scale&#39;] print(&#39;weights = {}&#39;.format(weights.data.numpy())) ## weights = [0.375 0.625] print(&#39;locs = {}&#39;.format(locs.data.numpy())) ## locs = [ 0.49887404 10.984463 ] print(&#39;scale = {}&#39;.format(scale.data.numpy())) ## scale = 0.6514337062835693 X = np.arange(-3,15,0.1) Y1 = weights[0].item() * scipy.stats.norm.pdf((X - locs[0].item()) / scale.item()) Y2 = weights[1].item() * scipy.stats.norm.pdf((X - locs[1].item()) / scale.item()) pyplot.figure(figsize=(10, 4), dpi=100).set_facecolor(&#39;white&#39;) pyplot.plot(X, Y1, &#39;r-&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x1a327303c8&gt;] pyplot.plot(X, Y2, &#39;b-&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x1a32730780&gt;] pyplot.plot(X, Y1 + Y2, &#39;k--&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x1a32730b70&gt;] pyplot.plot(data.data.numpy(), np.zeros(len(data)), &#39;k*&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x119f15f98&gt;] pyplot.title(&#39;Density of two-component mixture model&#39;) ## Text(0.5, 1.0, &#39;Density of two-component mixture model&#39;) pyplot.ylabel(&#39;probability density&#39;); ## Text(0, 0.5, &#39;probability density&#39;) "],
["reasoning-about-dags.html", "5 Reasoning about DAGs 5.1 Recap: Causal models as generative models 5.2 Reasoning with DAGs", " 5 Reasoning about DAGs 5.1 Recap: Causal models as generative models Our goal is to understand causal modeling within the context of generative machine learning. We just examined one generative machine learning framework called Bayesian networks (BNs) and how we can use BNs as causal models. 5.1.1 Ladder of causality There are three levels of causal inference and we call it the ladder of causality. Association (Seeing) Two variables are associative if observing one changes the probability of observing the other. Most of the machine learning models are good at finding an association between variables or features. Deep models are capable to find high-dimensional non-linear correlations. For Example, What does a symptom tell me about a disease? Association does not imply causality. In the above example, it might be that both the symptom and the disease both are caused by hidden variable(confounder) lifestyle. Intervention (Doing) In intervention, we override the normal causal structure, forcing a variable to take a value it might not have taken if the system were left alone. For Example, If I take aspirin, will my headache be cured? note that this distribution is difference than \\(P(cured\\mid aspirin)\\) because there might be a confounder cause. Interventions can be performed on any causal Bayesian networks. Here are a few examples of how intervention can be important for a machine learning project. Online Learning: Suppose owner of an online website finds out from his machine learning team that the predicted revenue for next month is lower than usual. He decides to run a Google Ad campaign to raise the revenue. By this way, he changes the process of how the revenue is driven. This is an intervention. If the machine learning team take this data to train their models for the next month, there will be an interference if they don’t account for the effect of ad campaign. Anomaly detection: Suppose a finance company is trying to build models to predict fraud. Based on the models, they reject the transactions that looks fraud. Now, if you retrain the model, the target becomes only the transaction that could outsmart your previous model. So, instead of conditioning on fraud, you should condition on the transactions that are fraud and able to outsmart the previous model. In both the cases, when some action is taken based on the outcome of the model, the new data is generated from a different distribution than the previous data generation process. Thus, it becomes a never ending loop of model update if we don’t consider the intervention. Counterfactuals (Imagining): Counterfactual reason about hypothetical situations, things that could happen if something was changed in past. The canonical interpretation of causality comes from Physics, where we can apply laws like gravitation and optics laws and predict the next state given the current state. But as human, we have a representation in our mind that can simulate and predict multiple scenarios had the initial conditions been different. For example, observing a pile of blocks at the edge of table, we can infer in which direction it will fall. If you want to build strong AI systems that reason like we reason, then you should be thinking about how to encode counterfactual reasoning in the system. Causal Bayesian networks allows us to build interventions where Structural Causal Models allows us to model both, interventions and counterfactuals. 5.1.2 Some definitions and notation This is the notation we will use throughout the course. We denote bold capital letters to denote a set of random variables. We use capital letters to denote a random variable. Let \\(\\mathbf{X}\\) be a set of random variables \\[\\mathbf{X} = \\{X_1, ..., X_d\\}\\] Joint probability distribution: \\[P_{\\mathbf{X}} = P(X_1, ..., X_d)\\] We use small letters to denote value of the variable or vector of random variables. So, in this case, let \\(x\\) be \\[ (\\mathbf{X}=x) := \\{X_1 =x_1, X_2=x_2, ..., X_d=x_d\\}\\] We denote density at \\(\\mathbf{X}=x\\) as, \\[P_{\\mathbf{X}=x} = \\pi(x_1, ..., x_d)\\] Let \\(P_{X_i}\\) be the marginal over \\(X_i\\), and \\(P_{X_i\\mid X_j}\\) be the conditional density. Let \\(\\mathbb{M}\\) be the generative model that entails joint distribution, either explicitly or implicitly. We denote the joint probability distribution “entailed” by a generative model as \\(P_{\\mathbf{X}}^{\\mathbb{M}}\\) Let \\(\\mathbb{G} = \\{V, E\\}\\) be the directed acyclic graph. Let the parents of \\(X_j\\) in the DAG \\(\\mathbb(G)\\) be \\(Pa_j^{\\mathbb{G}}\\) Below are some of models and programs we will use over and over, let’s review them. Bayesian network: A generative model that entails a joint distribution that factorizes over a DAG. Causal generative model: a generative model of a causal mechanism. Causal Bayesian network: is a causal generative model that is simply a Bayesian network where the direction of edges in the DAG represent causality. Probabilistic program: Generative model written as a program. Usually done with a framework that provides a DSL and abstractions for inference Causal program: Let’s call this a probabilistic program that As with a causal Bayesian network, you can write your program in a way that orders the steps of its execution according to cause and effect. 5.1.3 Difference between Bayesian networks and probabilistic programming Causal Bayesian network(BN) is a DAG, where each edge represents a causal effect between two nodes. In BNs, the joint distribution is a product of all the factored conditional probability distributions(CPDs). Probablistic Programming Langugage(PPL) is more expressive than Bayesian networks. Using PPLs you can develop none-parametric causal models like Dirichlet Processes. To understand this in detail lets take a look at the Chinese restaurant process example which is similar to k-means but without fixed value for k. The example is as follows: Imagine a Chinese restaurant in which customers enter. A new customer sits down at a table with a probability proportional to the number of customers already sitting there and sits on a new table with some probability. Bayesian networks cannot represent such a dynamic process with its static DAG. PPLs allow control flow (if, for, while) and recursion. which helps in creating open world model with variables that are avaible in models based on some condition. For example X = Bernoulli(p) if X == 1: Y = Gaussian(0, 1) Here, the existance of Y dependes on the value of X. You can also created complex model like gaussian random walk where each step depends on the previous step. X = Poisson(λ) Y = zeros(X) Y[0] = [Gaussian(0, 1)] for i in range(1, X): Y[i] = Gaussian(Y[i-1], 1)) Why bayesian networks over probabilistic programs? The application of graphical models come from its ability to perform inference. Inference in Bayesian networks is easy because of its constraints on types of models you can develop using DAG. Because of the intricacies of the control flow, in PPLs inference is tougher and hence users require some kind of inference expertise. That being said, PPLs provide inference abstractions and cutting-edge inference algorithms so users don’t have to work from scratch. Moreover, PPLs are backed by tensor-based frameworks like Tensorflow and PyTorch, that enables messive parallelism while performing inference. 5.2 Reasoning with DAGs DAG is a graphical language for reasoning about joint probability distribution, and also reasoning about causality. DAGs have been used to represent causal and temporal relationships between variables. We show how the approaches from probability and graph combines and give us a powerful language to reason about causality. 5.2.1 Probability concepts Conditional probability: Given two nodes \\(X\\) and \\(Y\\), conditional probability can be represented as: \\[P(X\\mid Y) = \\frac{P(X,Y)}{P(Y)}\\] Now rearranging, the joint can be expressed as \\[P(X,Y) = P(X\\mid Y)P(Y)\\] Conditional independence: Given that we have observed \\(Z\\), \\(X\\) is conditionally independent of \\(Y\\) in the probability distribution \\({P_{\\mathbb{A}}}\\)(denoted \\(X \\perp_{P_{\\mathbb{A}}} Y\\mid Z\\)), if and only if the conditional joint probability can be written as product of conditional marginal probabilities i.e, \\[P(X,Y\\mid Z) = P(X\\mid Z)P(Y\\mid Z)\\] Intuitively, this means that once \\(Z\\) is known, \\(Y\\) provides no additional information about \\(X\\). Thus, the joint distribution on \\(X\\), \\(Y\\) and \\(Z\\) is \\[P(X,Y,Z) = P(X,Y\\mid Z)P(Z) = P(X\\mid Z)P(Y \\mid Z) P(Z)\\] DAGs are useful for representing conditional independence relationship between variables. Lack of edges in DAG represent Conditional independence assumptions and hence, more such assumptions, lesser the edges in the graph. Conditional independence makes a DAG compact. 5.2.2 Graph concepts Path A path in \\(\\mathbb{G}\\) is a sequence of (at least two) distinct vertices \\(i_1,...,i_m\\), such that there is an edge between \\(i_k\\) and \\(i_k+1\\), for all \\(k=1,...,m-1\\). Pearl’s d-separation Consider three disjoint set of variables, X,Y and Z represented as nodes in a graph \\(\\mathbb{G}\\). To test whether X is independent of Y given Z, we need to test whether the nodes corresponding to variables \\(Z\\) blocks all paths from X to Y. This is defined by d-separation. Formally, a path \\(p\\) is said to be d-separated by a set of nodes Z if and only if: 1. \\(p\\) contains a chain \\(i\\to m\\to j\\) or a fork \\(i\\leftarrow m\\to j\\), such that the middle node \\(m\\) is in \\(Z\\) 2. p contains an inverted fork (or collide) \\(i\\to m\\leftarrow j\\) such that the middle node m is not in Z and such that no descendant of \\(m\\) is in \\(Z\\). A set \\(Z\\) is said to d-separated \\(X\\) from \\(Y\\) if and only if \\(Z\\) blocks every path from a node in \\(X\\) to a node in \\(Y\\). In the above picture, \\(U\\) is conditionally independent of \\(W\\), given \\(V\\) in the first three cases. Intuitively, in causal chains(1&amp;2) and causal forks(3), \\(U\\) and \\(W\\) are marginally dependent, but, become independent of each other when \\(V\\) is known. Conditioning on \\(V\\) appears to block the flow of information along the path, so learning about \\(U\\) will not effect the probability of \\(W\\), once \\(V\\) is observed. For example, in structure 1, consider \\(U\\) to be Grandparent’s genome, \\(V\\) the parent’s genome and \\(W\\) is your genome information, and we know everything about the parent’s genome(\\(U\\)). Now, there is no new information about your genome that your grandparent’s genome(\\(U\\)) can provide, given the parent’s genome(\\(V\\)). A similar blockage of information in observed in the second case. In structure 3(common parent), \\(V\\) is the parent’s genome, if \\(U\\) is the sibling’s genome, \\(W\\) is your genome. Now once the parent’s genome is know, theses no new information the sibling’s genome can provide that can explain your genome. V-structures V-structures, also know as colliders, or inverted forks, work in a different way. V-structures represents two causes having a common effect(structure 4 in the above figure). On observing the middle variable(effect \\(V\\)), the two extreme variables(causes \\(U\\) and \\(W\\)) which were marginally independent, will now have an unblocked path between them, making them dependent, and this is true for any descendant for \\(V\\) However, if the effect is not observed, the two variables causing it will remain independent. This is a little unintuitive, so let us consider a simple example of a sprinkler. Grass will be wet by two causes: when it rains(Rain = yes); when the sprinkler is on(sprinkler = on). Now lets say, we have observed that the grass is wet, and by some means(say, Google weather) we have the information that it has not rained(Rain = no). We now can conclude that the sprinkler was on(sprinkler = on). Generally, there is no correlation between rain and sprinkler, they are independent, but, when we observe the grass(the effect), the path is now unblocked, and this induces dependence among the causes(rain and sprinkler) This corresponds to the general pattern of causal relationships: observations on a common consequence of two independent causes tend to render those causes dependent, because information about one of the causes tends to make the other more or less likely, given that the consequence has occurred. There are two types of V-structures 1. immoral v-structure: V-structure in which the parents are not linked by an arc. 2. moral v-structure: V-structure in which the parents are linked by an arc. 5.2.2.1 What does conditional independence have to do with causality? Consider 2 variables \\(X\\) and \\(Y\\), a correlation between them would mean that either \\(X\\) causes \\(Y\\) or \\(Y\\) causes \\(X\\). Correlation implies that one of the two variables is causal. Now, consider a graph \\(\\mathbb{G}\\) with three variables, \\(X\\), \\(Y\\) and \\(Z\\) modeled as \\(X\\to Y\\to Z\\), whose joint probability can be factorized as \\[P(X)P(Y\\mid X)P(Z\\mid Y)\\] This can lead to three equivalent factorization: \\[P(X)P(Y\\mid X)P(Y\\mid X,Z)\\] \\[P(Y)P(X\\mid Y)P(Z\\mid Y)\\] \\[P(Z)P(Y\\mid Z)P(X\\mid Y)\\] And 3 equivalent DAGs. Now in such a case, correlation implies that one of these models, is a causal model. Using correlations, we may at least infer the existence of causal links from correlations, if not for a concrete causal graph. Conditional independence narrows down the causal negatives and reduces the problem to reasoning about the joint probability distribution to graph algorithms. R’s bnlearn library, includes a function d-sep, and Python’s pgmpy library with modules local_independencies and get_independencies, can be used to test for d-separation, or to get d-separated nodes. 5.2.2.2 Markov blanket The Markov blanket for a node in a graphical model contains all the variables that shield the node from the rest of the network. This means that the Markov blanket of a node is the only knowledge needed to predict the behavior of that node and its children. In terms of joint probability, this would mean that every set of nodes in the network is conditionally independent of \\(A\\), when conditioned on the Markov Blanket of \\(A\\). Formally, \\[{P(A\\mid \\operatorname {MB} (A),B)=P(A\\mid \\operatorname {MB} (A))}\\] Where \\({\\operatorname {MB}(A)}\\) is the set of nodes in the Markov Blanket of \\(A\\) In Bayesian networks, the Markov blanket of node A includes its parents, children and the spouses. In the above figure, the nodes in the blue circle is the Markov Blanket of node A. The reason why we include spouses because of the v-structure. Conditioning on the child, they become dependent The Markov Blanket, d-separates a variabe from everything else outside. This is an important concept to understand for machine learning as well. If we were fitting a model, once we include the Markov Blanket as predictors, any other predictor we add, is overfitting. Theoretically, markov blanket of a variable is the minimal set of predictors for that variable! 5.2.2.3 Markov Properties Global: A graph is globally Markov with respect to joint distribution if every d-sep inside the graph corresponds to conditional independence statement within the joint probability distribution \\({P_\\mathbb{X}}\\). Formally, \\[{U \\perp_{\\mathbb{G}} W\\mid V \\implies U \\perp_{P_\\mathbb{X}} W\\mid V }\\] Local: Every variable is conditionally independent of its non descendants given it parents. A well know example of local Markov property is a Markov chain. Markov factorization: If we can factorize a joint probability distribution by conditioning each node by its parents, then we satisfy Markov factorization property. This makes it a computational efficient way of evaluating the joint using logarithmic properties. \\[{P_{\\mathbf{X}=x}}=\\pi(x_1,...,x_d) = \\prod_{j=1}^{d} \\pi(x_j\\mid Pa_{j}^{\\mathbb{G}})\\] \\[\\log (\\pi(x_1,...,x_d)) = \\sum_{j=1}^{d} \\log \\pi(x_j\\mid Pa_{j}^{\\mathbb{G}})\\] These three properties are equivalent definitions, if one of them is true, the others are true. 5.2.2.4 Markov Equivalence Class Consider these valid factorizations of \\(P(A,B,C)\\) \\[\\begin{align} P(A)P(B\\mid A)P(C\\mid B)\\ (A\\to B \\to C)\\\\ P(C)P(B\\mid C)P(A\\mid B)\\ (C\\to B \\to A)\\\\ P(B)P(C\\mid B)P(A\\mid B)\\ (C \\gets B \\to A) \\end{align} \\] In all of these factorizations, we preserve the conditional independence of \\(A\\) and \\(C\\) (i.e. \\(A \\perp C \\mid B\\)). In other words, if you know that this conditional independence hold in the real world, only one of these three networks can be the true causal model. This is the reason why correlation does not imply causality. All of the factorizations above look the same from the statistical independence assumption. We often use PDAG (partial DAG) to show equivalence. All the members of the PDAG contains the same skeleton. Given an edge, for every graph in the PDAG family goes in one direction, that is a directed edge in PDAG. If there is at least one time when an edge goes in the opposite direction in the class, that becomes undirected. For example, second and third factorizations fall into \\(C - B \\to A\\) equivalence class. These three graphs can be factorized as a PDAG \\(C - B - A\\). PDAG gives a compact representation of an equivalance class. Once we have the PDAG, we can orient the undirected edges in any way, as long as we don’t introduce a new v-structure. So, every member of the equivalence class has the same v-structure. There are other graphical representations of joint probability distribution 1. Undirected graph - All edges are bidirectional, and does not admit causal reasoning. 2. Ancestral graphs - A type of mixed graph to provide a graphical representation for the result of marginalizing one or more vertices in a graphical model, this does not directly map to a generative model. 5.2.3 Faithfulness and Minimality These are the assumptions required to reason causally from a causal Bayesian network. Faithfulness Assumption: A distribution is faithful to a DAG if \\[U \\perp_{P_{\\mathbb{X}}} W | V \\implies U \\perp_{\\mathbb{G}} W | V\\] Minimality Assumption: A DAG is minimal with respect to a distribution if \\[U \\perp_{\\mathbb{G}} W | V \\implies U \\perp_{P_{\\mathbb{X}}} W | V\\] We talked about how d-separation assumptions make our DAG compact. A minimal DAG captures all the conditional independences in true distribution. "],
["introduction-to-interventions.html", "6 Introduction to Interventions 6.1 Ladder of causality 6.2 Assumptions for the Causality: Faithfulness and minimality 6.3 Structural causal models 6.4 Interventions", " 6 Introduction to Interventions In this lecture we will learn about the meaning of interventions and their implication to prediction. 6.1 Ladder of causality The ladder of causality has 3 levels: 1) Association 2) Intervention 3) Counterfactual. Most of the definitions and examples in this section are from the book of why by Pearl[2]. Figure 6.1: Ladder of causality. from the book of why by pearl. 6.1.1 Association The first level, Association is related to the cognitive ability of seeing or in other words observation. It includes broad class of statistically learned models (discriminiative and generative). In this level, we are looking for regularities in observations. This is what an owl does when observing how a rat moves and figuring out where the rodent is likely to be a moment later, and it is what a computer Go program does when it studies a database of millions of Go games so that it can figure out which moves are associated with a higher percentage of wins. We say that one event is associated with another if observing one changes the likelihood of observing the other. In the association level of the ladder of causality, we call for predictions based on passive observations. As it is noted in figure 6.1 this level is characterized by the question “what if I see …?” For example one example in this case is how likely is a customer who bought toothpaste to also by dental floss? This can be easily calculated in Statistics by gathering data on the customers who bought toothpaste and then among those focus on the ones who also bought dental floss. In other words, we want to calculate the conditional probability of \\(P(floss = 1 | toothpaste = 1)\\). This will be calculated based on observational data or in other word based on what was seen. Statistics alone cannot tell whether buying floss is the cause of buying the toothpaste or vice versa. For the sales manager it really doesn’t matter what is the causal relationship between this two items. Another example of association is “correlation” and “regression” which is a typical measure of association. Most of machine learning methods are also in the first ladder of causality like deep neural networks. In deep neural networks We are looking for a way to learn the association in a high dimensional non linear space. Deep learning has given us machines that have impressive abilities but no intelligence. They are driven by a stream of observations (raw data) to which they attempt to fit a function, just like how we fit a line in linear regression. We can think of Association in terms of discriminative models. One common example of Association is Neural Network. Neural network brute forces things; the goal of a neural network is to find the right set of weights to bridge the input and output. Another example of association is the Naive Bayes classifier. 6.1.2 Intervention The second level of ladder of causality is intervention. It is when we begin to change the world. For the example of toothpaste and floss a question to ask for this level would be, “What will happen to our floss sales if we double the price of toothpaste?” This knowledge is absent from raw data and we cannot answer this question from passively collected data (raw data). In this case you want to deliberately intervene to the price of toothpaste regardless of any market conditions that may had affected the price in observational data (in your observational data you may see doubled prices for toothpaste as well as it’s original price but that is due to market conditions like lack of toothpaste for a period of time). One way to predict the result of an intervention is to experiment with it under carefully controlled conditions. Even if we don’t have an experiment to predict interventional results, if we have an accurate causal model, we are able to move from level 1 of the ladder to level 2. Later in this section we will talk about structural causal models (SCMs). Level 2 of ladder of causality is characterized by the question “What if we do …?” What will happen if we change the environment? In pearl’s notation, for our thoothpaste example we write this kind of query as \\(P(floss | do(toothpaste))\\). This tells us the probability that we will sell floss at a certain price, given that we set the price of toothpaste at another price. Note that this do operator is different from conditional probability. We will talk more about this in the next section. 6.1.3 Counterfactual Level 3 of ladder of causality is characterized by the question “What if I had done …?” “Why?”. A good example of a counterfactual question can be “Had Trump not fired Comey, would he have still been charged with obstruction of justice” to which Muller responded with another counterfactual statement. He said that if he did believe that a crime was committed, he would have said so. No experiment can go back in time to see the effect of a different treatment on the patient when all other conditions are fixed. The data that we have cannot tell us what will happen in a counterfactual or imaginary world, but the human mind makes these inferences reliably all the time. Causal Bayesian networks cannot answer counterfactual questions. For toothpaste example, we can ask “What is the probability that a customer who already bouth toothpaste would still have bought it if we had doubled the price?”. Having a causal model that can answer counterfactual questions is very valuable. We call these models Structural causal models (SCMs) that can be implemented in a probabilistic programming language. Structural causal models look like a causal Bayesian network except all the variables in the joint distribution are going to be a deterministic function derived from its parents and some noise. 6.2 Assumptions for the Causality: Faithfulness and minimality Suppose the set of observed variables \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) is causally sufficient and its causal structure can be properly represented by a DAG over V. Causal Minimality Assumption: Every d-separation statement entailed by the causal DAG over V is satisfied by conditional independence over V. Minimality is the property of DAG. In other words, we say that a DAG is minimal with respect to the entailed distribution. Causal Faithfulness Assumption: Every conditional independence statement is d-separated by the causal DAG over V. Minimality is the property of the factorization of the joint. In other words, we say that a the factorization is faithful to the DAG. 6.3 Structural causal models Structural causal model (SCM) implies the nature of data generation procedure, which has both deterministic process and random variation. Randomness comes from the initial conditions and the relationship between variables is deterministic. The random initial variables are often called noise variables or exogenous variables. Every variable in the system is associated with its exogenous noise variable. Here is an example. \\[ N_X\\sim\\mathcal{N}(0,1), N_Y\\sim\\mathcal{N}(0,1)\\\\ X = N_X\\\\ Y = X^2+N_Y \\] In general, structural causal model takes the form \\[ C :=N_C\\\\ E := f_E(C, N_E) \\] Here \\(N_C\\) and \\(N_E\\) are noise variables. \\(N_C \\perp N_E\\). \\(f_E\\) is the deterministic functional assignment. In practice, this can be arbitrary functional approximator, like neural networks. Here is a causal program for the first example in Pyro. This program can be a model of Newtonian mechanics which accepts all the noise as pre-conditions and hence becomes determnistic. def fx(Nx): return Nx def fy(X, Ny): return X*X + Ny def scm(noise): N_X = noise[x].sample() N_y = noise[y].sample() X = fx(Nx) Y = fy(X, Ny) The idea of SCMs is very much aligned with a famous thought experiment called Laplace’s demon - If the demon knows the precise initial conditions of every atom in the universe, their past and future values for any given time are entailed; they can be calculated from the laws of Newtonian mechanics (deterministically). Considering a simple linear regression case, \\[X = \\pi_X \\\\ \\epsilon = N(0,1) \\\\ Y= \\beta X + \\alpha + \\epsilon\\] In this case, the data generation process of \\(X\\) has no deterministic mechanism. In a slightly different case, \\[N_x = \\pi_x \\\\ X = F(N_x) \\\\ \\epsilon = N(0,1) \\\\ Y= \\beta X + \\alpha + \\epsilon\\] In this case, the data generation process has both deterministic mechanism (i.e. \\(F(N_x)\\)) and random process on X. 6.4 Interventions In this section, we formally introduce interventions and demonstrate different ways to perform intervention on Bayesian netowrks as well as SCMs. 6.4.1 Relationship between intervention and causation There is a famous quote that say “correlation is not causation”. If two variables are correlated, one is not necessarily the cause of the other one. For example ice cream sale and rate of crime are highly correlated with each other, but none of them is the cause of the other one. In fact, ice cream and violent crime are more common in hot weather. For this reason, in order to find causality, randomized controlled experiments are useful tools. In these experiments, all factors that influence the outcome variable are static, or vary at random, except for one variable. So if the outcome variable changes, it means that the outcome variable must be due to that one input variable. We connot design a randomized controlled experiment for all the cases. For example we cannot control the weather or it is not to control for some factors like force people to smoke to experiment it’s effect on lung cancer. In some cases, even randomized control trials may encounter problem. People may drop out of trial or fail to take their medication. Intervention helps to find causal relationship. Considering a simple case where A and B are only two nodes in BayesNet. Their association implies either A causes B or B causes A. With intervention of A, if probability of B changes, then A is the cause, otherwise B is the cause. There is a lot of philosophical controversy associated with the intervention and causation. Paul Holland firmly believes that “There is no causation without intervention”. This implies that we cannot simply say that obesity is the cause of heart attack since obesity cannot be meaningfully intervened (i.e. Obesity can’t be set individually without perturbing other factors that leads to obesity). Only factors that can be meaningfully intervened qualify as a cause of a given outcome. We will not get into the phosophical debate of the validity of these different views on causality. But it is important to note that they exist and they’re controvertial. 6.4.2 Relationship between intervention and prediction Let’s remind ourselves of why prediction is important in the context of machine learning. You train your model with your data, and now you have a predictor. The next step is to make decisions based on predictions. For example when you predict that someone has cancer with your machine learning algorithm, it’s not enough to only predict the deases and let the patient alone! You have to make a decision for the patient about what to do next! Most of the times, the decisions are downstream of predictions. If we train a model that takes data about weather and predicts whether or not it is going to rain tomorrow, and everyday that I have new data I retrian the algorithm. If I get up in the morning and the algorithm predicts that it is going to raing today, the decision that I make is to take an umbrella! In this example, my decision doesn’t have any effect on the weather. Consider another example. If I work in a commerce company and I want to predict what my revenue today based on the last few days of online transactions. If I think that it is going to be low, my decision would be to launch some facebook ads. The facebook ads are going to drive traffic and hopefully impact myself. This decision is going to influence data point and feed it back into my training data. This is an example of intervention. Reasoning of intervention allows us to predict outcome, when it is not feasible to do the intervention. One can calculate the theoretical effect on an intervention without actually doing anything. It is important to understand the implications of an intervention to prediction. Consider the examples that we just discussed: predicting the weather vs predicting the sales. It makes more sense to apply an intervention to predict the sales than to predict the weather, since sales but weather may be changed by intervention. Intervention can be used to predict the effect of a particular ad campaign on sales. One of the features of intervention here is the action may be real or hypothetical. 6.4.3 Types of Intervention and its representation on DAG There is a difference between intervening on a variable and conditioning on that variable. If we intervene on a variable, we fix its value but the values of other variables change. When we condition on a variable, we only narrow our focus to the subset of cases where the variable we condition on takes a specific value. perfect intervention: is to artificially assign a value to a random variable. This means that once an intervention is performed on that random variable, its value or outcome will be deterministic, regardless of other parents. Sometime it can be awkward to represent in terms of conditional probability or even more awkward for continuous random variables. Perfect interventions could be viewed as graph mutilation, when “mutilate” the DAG means removing incoming edges the intervened upon variable. If original DAG has Markov property, then mutilated graph also follows the same properties. If causal model is correct, then you can predict outcome of intervention without really doing experiment. The other way to show perfect intervention graphically in a causal Bayes net is to draw a node \\(I\\) with no causes and two states (on/off) that goes to the variable that is intervened. For example, consider figure 6.2 where X –&gt; Z. Figure 6.2: Intervention on Z with node I We can show this in this table. The first four rows are when the state of intervention if off and the last four rows is when the sate of intervention is on and Z gets value one with probability one. | I | X | Z | Prob | |---|-------------|---|------| | 0 | 0 | 0 | .8 | | 0 | 0 | 1 | .2 | | 0 | 1 | 0 | .1 | | 0 | 1 | 1 | .9 | | 1 | 0 | 0 | 0 | | 1 | 0 | 1 | 1 | | 1 | 1 | 0 | 0 | | 1 | 1 | 1 | 1 | Mathmatically, it could be presented as below, \\[Z \\sim \\left\\{\\begin{matrix} \\text{Normal}(\\beta X + \\alpha, 1) &amp; \\text{, if I} = 0 \\\\ \\text{Dirac}(z) &amp; \\text{, if I} = 1 \\end{matrix}\\right.\\] In a probabilistic program, it could be presented as, z = 0 # or some other intervention value def program(I): X ~ Normal(0, 1) if I: Z ~ Normal(beta X + alpha, 1) else: Z ~ Dirac(y) # or just `Z = z` Another example that shows intervention graphically is as below: Figure 6.3: DAG An intervention on B would look like this: Figure 6.4: Intervention_DAG Soft intervention: IN Soft intervention, we intervene on variable by changing it’s distribution. Soft intervention on \\(X\\) doesn’t yield a deterministic assignment to \\(X\\), instead it changes the way \\(X\\) was originally generated. For example, \\[Z \\sim \\left\\{\\begin{matrix} \\text{Normal}(\\beta X + \\alpha, 1) &amp; \\text{I} = 0 \\\\ \\text{Normal}(\\sigma X + \\alpha, 1) &amp; \\text{I} = 1 \\end{matrix}\\right.\\] Randomization could be one special case of intervention which aims to break influence of latent confounders. We will dive deeper into confounders in later and will also formally define randoization as intervention. Other way of implementation can be probabilistic program using metaprogramming technique, such as do-operater in pyro. 6.4.4 Interventions as Pearl’s Do-calculus In notation, when show forcing a variable \\(X\\) to take a value \\(x\\), by \\(do(X=x)\\). So, \\(P(Y = y | X = x)\\) is the probability that \\(Y = y\\) conditional on finding \\(X = x\\), but \\(P(Y = y | do(X=x))\\) is the probability that \\(Y = y\\) when we intervene to fix \\(X=x\\). do-operater could be used as metaprogramming in Pyro. It is the same as perfect (or “ideal”) intervention. \\(p(y|do(x))\\) answers the question about what is the distribution of Y if I were to set the value of X to x. In the distributional terminology, \\(P(Y = y|X = x)\\) is the population distribution of Y among individuals whose \\(X\\) value is \\(x\\). But, \\(P(Y = y|do(X = x))\\) represents the population distribution of Y if everyone in the population had their X value fixed at x \\(^{[1]}\\). \\(P(Y|X)\\) is an example of Association and the data is generated by performing rejection sampling until the desired number of samples is retrieved. There is no change in the original distribution of any of the random variables. In contrast, \\(P(Y|do(X))\\) is an intervention where the value of X is deterministic and becomes independent of its parents. Therefore, the resultant data from \\(P(Y|X)\\) and \\(P(Y|do(X))\\) is different. Example: SCM One case using pyro as representation is: def program: A = flip($P_A$) B = flip($P_{B1}$) if A else Flip($P_{B2}$) if A C = flip($P_{C1}$) if B else Flip($P_{C2}$) else C = flip($P_{C3}$) if B else Flip($P_{C4}$) To get the probability of C conditioned on B = 1: C_prog = pyro.condition(program, {B=1}) An intervention would result in the following changes def intervened_program: A = flip($P_A$) B = Dirac(1) if A C = flip($P_{C1}$) if B else Flip($P_{C2}$) else C = flip($P_{C3}$) if B else Flip($P_{C4}$) This program is same as when we do(B) = 1: intervene_prog = pyro.do(program, data = {B=1}) Example: Randomized Controlled Trial Another example would be that of cancer treatment. There are two treatments for cancer - Treatment A and B. Treatment A is the treatment whose effect is to be found out and Treatment B is a placebo. The DAG may look something like this: Figure 6.5: Cancer treatment Here T represents the treatment and O represents the outcome. Z represents some latent confounder that affects the treatment as well as the outcome (something like diet or exercise). An intervention would mutilate the graph and cut off all it’s parents. In this case, an intervention would be something like a coin toss where if the outcome of the coin toss is heads the person would receive treatment A or else treatment B. This is an excellent example of using randomization as a perfect intervention because the outcome does not depend on any latent confounders. Figure 6.6: Cancer treatment intervention It allows us to reason about interventions, when it is not feasible to do the intervention. Another advantage of using do-operator is calculating the theoretical effect on an intervention without actually doing anything. References [1] Judea Pearl, Madelyn Glymour, Nicholas P. Jewell. Causal Inference in Statistics: A Primer [2] Judea Pearl, Dana Mackenzie. The Book of Why: The New Science of Cause and Effect "],
["calculating-intervention-distributions-by-covariate-adjustment.html", "7 Calculating intervention distributions by covariate adjustment 7.1 Review of causal sufficiency and interventions 7.2 Review of intervention 7.3 Intuition about experimental effects and confounding 7.4 Graph-based 7.5 Calculating intervention distributions by covariate adjustment 7.6 Truncated formula AKA g-formula 7.7 Valid adjustment sets", " 7 Calculating intervention distributions by covariate adjustment 7.1 Review of causal sufficiency and interventions Global Markov property Causal faithfulness Causal minimality Faithfulness implies causal minimality 7.2 Review of intervention Intervention is an artificial manipulation of the DAG. Interventions change the joint distribution such that the intervened variable no longer correlates with its causes. Randomized experiments are a type of intervention. Ladder of causality – How can we predict interventions? Associational models don’t have an a way of calculating how the joint distribution changes under intervention. Best you can do is actually perform interventions and include the intervention data in your training data. Causal models allows us to predict the effect of an intervention. Why would we want to predict an intervention? If a randomized experiment is a type of intervention, then you might ask why we would want to predict the outcome of a randomized experiment. Randomized experiment may be costly in terms of time and resources. They may be impossible or unethical to run. But wait, why do we run randomized experiments again? 7.3 Intuition about experimental effects and confounding Confounding example 1 I want to know the effect someone screaming “fire” in a movie theater has on making people run for the fire exit. But when people usually scream fire, there is usually a fire. When people run to the exit, are they responding to the scream or to the fire? So here we are interested in a direct relationship between (scream and fire), and are having troubling seperating it from indirect relationship between screaming and the actual presence of a fire, as well as people running and the actual effect of a fire. Confounding example 2 Running an A/B test. So on Monday morning, you send all the users to A. On Monday evening, you send all the users to B. You have asked a question of the universe: What the difference is between A and B under the conditions of the experiment? Key word here is “difference” \\[ \\begin{align} &amp; P(R = 1 | T = a) - P(R = 1 | T = b) \\nonumber\\\\ =&amp; \\sum_{z}P(R = 1 | T = a, Z=z)P(T=a|Z=z)P(Z=z) - \\sum_{z}P(R = 1 | T = b, Z=z)P(T=b|Z=z)P(Z=z) \\nonumber \\end{align} \\] These probabilities vary depending on what level of z we are looking at. We know what is wrong with our experiment – randomization. How does it fix this problem? Can you explain it without graphs? Language problem Statistics cannot define the term “confounding”, need a causal grammar. Similarly, interpret the differences between treatment populations. 7.4 Graph-based Confounding bias occurs when a variable influences both who is selected for the treatment and the outcome of an experiment. Sometimes they are known, sometimes they are latent. Contrast this with a latent variable model where we are typically trying to infer the state of the latent. What about if we want to predict the latent, but there is another confounder? Topic model example What does it mean to “control for something”? In terms of tables? 7.5 Calculating intervention distributions by covariate adjustment We have a causal model \\(\\mathbb{M}\\) that entails the joint distribution \\(\\mathbb{P}\\). Our model is a machine that gives us joint, conditional, or marginal probability of any outcome (though we may need an inference algorithm to compute the probability). Recall local Markov property: \\[p^{\\mathbb{M}}(x_1, ..x_d)= \\prod_{j=1}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)})\\] Our causal model’s DAG gives use this factorization. The intervention question: if an intervention changes joint distribution, how can we calculate the results of an intervention without actually having to do the intervention? Covariate adjustment: calculating the results of interventions without do-calculus (graph mutilation). Motivation? The Do-calculus is a tool. We better understand the power of a tool and how it works if we understand exactly what we can accomplish without the tool. Much of the causal inference community doesn’t use do-calculus, but they do use covariate adjustment. Will give us a better understanding of confounding – we said a RCT is a c Given a model entailing \\(\\mathbb{M}\\), if we apply an intervention to the model and acquire \\(\\mathbb{\\tilde{M}}\\), then \\[\\pi_{\\mathbb{M}}(x_j | x_{\\text{pa(j)}}) = \\pi_{\\mathbb{M}}(x_j | x_{\\text{pa(j)}})\\] 7.6 Truncated formula AKA g-formula 7.6.1 Invariance property of interventions Assume we have a model \\(\\mathbb{M}\\), that factorizes according to some DAG. Then by the local Markov property: \\(p^{\\mathbb{M}}(x_1, ..x_d) &amp;= \\prod_{j}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)})\\), where \\(x_{pa(j)}\\) is a vector of values for the parents of \\(X_j\\) in the DAG. So we know the values for each factor \\(p^{\\mathbb{M}}(x_j|x_{pa(j)})\\) ( – that’s part of what the model encodes. Again, what we don’t know is what the new distribution under intervention is going to be. Let \\(\\mathbb{\\tilde{M}}\\) be the mutated (mutilated) model we get after we apply a soft intervention \\(do(X_k := \\tilde{N})\\), where \\(\\tilde{N}\\) has a probability density function \\(\\pi\\). Then according to the local Markov property. \\[ \\begin{align} p^{\\mathbb{\\tilde{M}}}(x_1, ..x_d) &amp;= p^{\\mathbb{M}; do(X_k = \\tilde{N})}(x_k) \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) \\nonumber \\\\ &amp;= \\pi(x_k) \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) \\nonumber \\end{align} \\] The interventions changes only the factor \\(p^{\\mathbb{M}}(x_k|x_{pa(k))}\\) – it becomes \\(\\pi(x_k) = p^{\\mathbb{M}; do(X_k = \\tilde{N})}(x_j)\\), which no longer depends on the parents \\(x_{pa(j)}\\). The key thing here is that, all of the factors from \\(\\mathbb{M}\\) are the same in \\(\\mathbb{\\tilde{N}}\\). In the special case of a hard intervention, this simplifies to \\[ \\begin{align} p^{\\mathbb{M}; \\text{do}(X_k = a)}(x_1, ..x_d) = \\left\\{\\begin{matrix} \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) &amp; \\text{if} X_k = a \\\\ 0 &amp; \\text{otherwise} \\end{matrix}\\right. \\nonumber \\end{align} \\] 7.6.2 Conditioning and do are the same for variables without parents Consider what would happen if \\(x_k\\) had no parents? \\[ \\begin{align} p^{\\mathbb{M}}(x_1, ..x_d|X_k =a) &amp;= \\frac{\\prod_{j}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) }{P^{\\mathbb{M}}(x_k=a)}\\nonumber \\\\ &amp;= \\frac{p^{\\mathbb{M}}(x_k|x_{pa(k)}) \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)})}{P^{\\mathbb{M}}(X_k=a)} \\nonumber \\\\ &amp;= p^{\\mathbb{M}}(x_k) \\prod_{j \\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) \\nonumber \\\\ &amp;= \\left\\{\\begin{matrix} \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) &amp; \\text{if} X_k = a \\\\ 0 &amp; \\text{otherwise} \\\\ \\end{matrix}\\right. \\nonumber \\\\ &amp;= p^{\\mathbb{M}; \\text{do}(X_k = a)}(x_1, ..x_d) \\end{align} \\] 7.7 Valid adjustment sets Valid adjustment sets, and the problem of over controlling. Even amongst statisticians, knowing what to control for, or what confounding is, has been a problem. This motivates all the work we are doing in parsing DAGs. Parsing Ezra Klein TODO Gender wage gap example. “The question to ask about the various statistical controls that can be applied to shrink the gender gap is what are they actually telling us… The answer, I think, is that it’s telling how the wage gap works.” One should not control for things that are part of the causal mechanism. \\(p^{\\mathbb{M}; X =x}(y)&amp;= \\sum_{z} p^{\\mathbb{M}; X =x}(y|x, z)p^{\\mathbb{M}}(z)\\) Parent adjustment Backdoor criterion Toward neccessity Front door If we do not observe the latent, we can’t use the back-door, but we can: \\(p^{\\mathbb{M}; \\text{do}(X=x)}(y)= \\sum_z p^{\\mathbb{M};X=x}(z) \\sum_{\\tilde{x}} p^{\\mathbb{M};X=\\tilde{x}Z=z}(y) p^{\\mathbb{M}}(\\tilde{x})\\) "],
["confounding-paradoxes.html", "8 Confounding, Paradoxes 8.1 A second look at confounding 8.2 Monte Hall 8.3 Berkson Paradox 8.4 Examples of valid adjustment 8.5 Covariate adjustment: Simpson’s Paradox 8.6 Front-door adjustment 8.7 Propensity score 8.8 Covariate adjustment example: Continuous adjustment", " 8 Confounding, Paradoxes 8.1 A second look at confounding Confounding Interested in an average treatment effect. In context of randomized A/B, this is the difference on average between outcome under group A and outcome under group B More generally, the average difference between interventions. So we want this intervention distribution. The adjustment criterion tell us what variables we can control for. We think of these in terms of sets, because some confounders may be hidden. It may not be possible to control for everythings The adjustment formula tells us that once we have the covariate-adjustment formula, we can estimate this intervention distribution. 8.2 Monte Hall Describe problem Game show Choose a door Monte will open a door that does not have the car Should always pick. Monty Hall must open a door that does not have a car behind it chosen door -&gt; door open &lt;- location of the car (scribe create figure) Door opened is a collider 8.3 Berkson Paradox Two features seem to have no relation to each other in general, they can appear to be associated within a context This is the core of sampling bias 8.4 Examples of valid adjustment 8.5 Covariate adjustment: Simpson’s Paradox You are a data scientist at a prominent tech company with paid subscription entertainment media streaming service. You come across there results of an A/B test that a rival data scientist ran. The test targeted 70K subscibers users who were coming to a subscription renewal time and were at high risk of not renewing. They were targeted with either treatment 0 - a personalized promotional offer that gave the user reduced rates on the media they consume the most, or treatment 1 - a promotional offer that gave the user reduced rates on a general set of content. Overall Treatment 0: Personalized Promotion 77.9% (27272/35000) Treatment 1: Generalized Promotion 82.6% (28902/35000) This reads that 78% of the users who recieved the personalized promotion ended up renewing their subscription, while 83% of those who recieved the generalized promotion ended up renewing. Let R be the 0 if a subscriber leaves, and 1 if a subsciber stays. In his report, the analyst quantified the effect size as: \\[ E(R | T = 0) - E(R | T = 1) = P(R = 1 | T = 0) - P(R = 1 | T = 1) \\approx .779 - .826 = -0.047 \\] … where .779 and .826 are empirical estimates. So the conclusion was that the probability a subscriber stays is nearly .05 higher on the generalized promotion than on the personalized promotion. The marketing executives took this as a no-brainer. While the effect size is small, the p-value was near 0 so these results were significant (never mind that this was simply because sample size is large, which is typically the case in tech). Users generally prefer high quality content, and high quality content generally has higher royalty costs. So personalized promotions are typically more expensive than the generalized ones, where you can mix less popular but more cost-effective content into the promotion. So if the generalized promotion is cheaper AND performs better in the test, then its clearly better choice as a policy for dealing with subscribers who at high risk of leaving… Right? Curious, you find the SQL query that generated the data, and play around with selecting a few more columns and joining a few other tables. You notice that for these subscribes, there was also data on how happy the customers were, based on interactions with customer service. You create a new table that works in this new Z(not disgruntled/disgruntled) variable. Overall Not disgruntled Disgruntled Treatment 0: Personalized Promotion 77.9% (27272/35000) 93.2% (8173/8769) 73.3% (19228/26231) Treatment 1: Generalized Promotion 82.6% (28902/35000) 86.9% (23339 / 26872) 68.7% (5582/8128) Lo and behold, the conclusion is reversed within each level of Z! While generalized promotion seems favorable relative to personalized promotion in general, the personalized promotion seems to perform better within each of the non-disgruntled and disgruntled subgroups. This turns out to be an example of Simpon’s paradox. Suppose the true underlying model has the following DAG: simpson_model …where Z is 0 for non-disgruntled, 1 if disgruntled. Consider two SCMs \\(\\mathbb{C}^{do(T:=0)}\\) and \\(\\mathbb{C}^{do(T:=0)}\\) that are obtained by interventions setting \\(T := 0\\) and \\(T := 1\\). Let \\(P^{\\mathbb{C};do(T:=0)}\\) and \\(P^{\\mathbb{C};do(T:=0)}\\) denote the probability distributes entailed by these SCMs. Calculate the expected difference in outcome between the two treatments using the empirical counts in the above table $ E^{T=0}(R) - E^{T=1}(R)$. Calculate analytical the Average Treatment Effect using the empirical values in the table: \\[ \\begin{align*} ATE &amp;= E^{do(T:=0)}(R) - E^{do(T:=1)}(R)\\\\ &amp;= P^{\\mathbb{C};do(T:=0)}(R=1) - P^{\\mathbb{C};do(T:=1)}(R-1)\\\\ \\ \\\\ P^{\\mathbb{C};do(T:=0)}(R=1) &amp;= \\sum_{z=0}^1 P^{\\mathbb{C};do(T:=0)}(R=1, Z=z) \\\\ &amp;= \\sum_{z=0}^1 P^{\\mathbb{C};do(T:=0)}(R=1, T=0, Z=z) \\\\ &amp;= \\sum_{z=0}^1 P^{\\mathbb{C};do(T:=0)}(R=1| T=0, Z=z) P^{\\mathbb{C};do(T:=0)}(T=0, Z=z) \\\\ &amp;= \\sum_{z=0}^1 P^{\\mathbb{C};do(T:=0)}(R=1| T=0, Z=z) P^{\\mathbb{C};do(T:=0)}(Z=z) \\\\ &amp;= \\sum_{z=0}^1 P^{\\mathbb{C}}(R=1| T=0, Z=z) P^{\\mathbb{C}}(Z=z) \\\\ &amp;\\approx .932 * 35641/70000 + .733 * 34359/70000 = .8343 \\ \\\\ P^{\\mathbb{C};do(T:=1)}(R=1) &amp;\\approx .869 * 35641/70000 + .687 * 34359/70000 = .7818 \\ \\\\ \\ \\\\ ATE &amp;= .8343 - .7818 = 0.0525\\\\ \\end{align*} \\] 8.6 Front-door adjustment Front-door criterion: A set of variables Z is siad to satisfy the front-door criterion relative to an ordered pair of variables (X, Y) if Z intercepts all directed paths from X to Y There are no unblocked paths from X to Z All backdoor paths from Z to Y are blocked by X You are a data scientist investing the effects of social media use on a purchase. You assume the following DAG Circles mean unobserved, squares mean observed In this model the causal effect of social media on conversions is not identifiable; one can never ascertain which portion of the observed correlation between X and Y is attributed to user context U. It is worth noting that there are ways of analyzing how strong that confounding effects must be in order to entirely explain the association between X and Y Now suppose you modify the SQL query and get an additional variable: whether or not the person was using an ad blox. In this case we can apply the front-door criterion. Assume that were query the database for a past experiment where a randomly selected sample of 800000 “whales” – tech lingo for users who generally have a high conversion rate (because of evironmental factors, like their generation or social/professional in-group). Assume the following tqble (blackboard) One person on your team argues that the table proves that social media does not drive conversions. They point to the fact that only 15% of people who converted used social media, compared to 92.25% of people who don’t use social media. Another member oof your team argues that social media use actual increases, not decreases, conversions. Their argument is as follows: If you use social media, then your chances of seeing a high level of ads is 95% (380/400) compared to 5% if you do not use social media (20/400). The effect of ad exposure, if we look seperately at the two groups, social media users and non-users in in the second table (blackboard). In social media users it increases conversion rates from 10% to 15% in non-social media users it increases conversion rates from 90 to 95% Here is how we break the stalemate with a technique called the front-door formula: First, we see that the effect of X on Z is identifiable because there is no backdoor path from Z to X. \\(P(Z = z|do(X = x))\\) = P(Z = z|X = x) Next, we not that the effect of Z on Y is identifiable. The backdow path from Z to Y, namely Z &lt;- X &lt;- U -&gt; Y can conditioning on X. \\(P(Y = y|do(Z = z)) = \\sum_x P(Y = y|Z = z, X=x)P(X = x)\\) We chain toghether these two parital effects to obtain the overall effect of X on Y. If nature chooses to assign Z the value z, then the probability of Y would be \\(P(Y=y | do(Z = z))\\). The probability that nature would choose to do that, given that we choose to set X to x is \\(P(Z = z|do(X = x))\\) Summing up over all the possible states z of Z we have \\[ P(Y = y|do(X = x)) = \\sum_Z P(Y = y |do(Z = z))P(Z =z|do(X = x)) \\] Finally, we replace the do-expressions with their covariate adjustment counterparts. The final expression is \\[P(Y = y|do(X = x)) = \\sum \\sum_{x&#39;} P(Y = y|Z = z, X = x&#39;)P(X = x&#39;)P(Z = z|X = x)\\] 8.7 Propensity score Consider the following case of confounding: The set {Z1, Z2, Z3} is a valid adjustment set by parent adjustment So we could estimate the intervention distribution using \\(p^{M; do(X:=x)}(y) = \\sum_{z_1, z_2, z_3}p^{\\mathbb{M}}(y|x, z_1, z_2, z_3)p^{\\mathbb{M}}(z_1, z_2, z_3)\\) Here we consider the case where Z1, Z2, and Z3 there exists some function \\(L(Z_1, Z_2, Z_3)\\) that renders X conditionally independent from Z1, Z2, Z3, i.e. $ X{ Z_1, Z_2, Z_3 } |L(Z_1, Z_2, Z_3)$ To help imagine this, it modify our causal model to add \\(L(Z_1, Z_2, Z_3)\\) to the graph as a type of continuous causal AND gate as in . Let \\(l\\) represent a value codomain of \\(L(.)\\) Our new adjustment formula is \\(p^{M; do(X:=x)}(y) = \\sum_{l}p^{\\mathbb{M}}(y|x, l)p^{\\mathbb{M}}(l)\\) What have we gained here? \\(p(y |x, l)\\) is potentially of lower dimention that \\(p(y|x, z_1, z_2, z_3)\\). It might be computationally easier to estimate \\(p^{M; do(X:=x)}(y)\\) with covariate adjustment over \\(L(.)\\) than over Z1, Z2, Z3. In industry and practice, you often hear of “propensity matching”, where you try to group examples that have similar values of \\(L(.)\\) and then calculste cause effects between X and Y within those groups. In other words, you control for/condition on/adjust for L Next class (scribes delete) * Structural causal models * Instrumental variables * Potential outcomes * Inverse proability weighting * mediation 8.8 Covariate adjustment example: Continuous adjustment Since we are talking differences, what might this look like in the continuous case? \\(\\frac{d}{dx}E^{\\mathbb{M};do(X:=x)}(Y)\\) * Linear case – Z in valid adjustment set * Nonlinear case: Monte Carlo Sampler. Recall that. \\(\\frac{d f(x)}{dx} = \\lim_{\\delta \\rightarrow 0} \\frac{f(x + \\delta) -f(x)}{\\delta}\\) "]
]
